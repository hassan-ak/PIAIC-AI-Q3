{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1gK60vg-UUI"
      },
      "source": [
        "# Binary text classification - IDMB - UNIGRAMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqg39x0uCiO8",
        "outputId": "29ca1b70-6d49-4ad3-8730-91809c891c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9855k      0  0:00:08  0:00:08 --:--:-- 16.3M\n"
          ]
        }
      ],
      "source": [
        "# download an unzip data\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3noE22KgDN34"
      },
      "source": [
        "- 25,000 text files for training and another 25,000 for testing\n",
        "  - 12500 +ve\n",
        "  - 12500 -ve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6KqEermDSv-"
      },
      "outputs": [],
      "source": [
        "# remove un-necessary folder\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZphNFmp_DgxP",
        "outputId": "9cc6f760-50d3-4d2d-b50e-b31a4a340119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "# inspect data\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICOEtnsrEDfT"
      },
      "outputs": [],
      "source": [
        "# prepare a validation set \n",
        "  # by setting apart 20% of the training text files in a new directory, aclImdb/val\n",
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AH3xzSZENTN",
        "outputId": "544fb9d5-2819-4af2-d85c-96442038c133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# create text datasets\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvt0QwZaFJV_",
        "outputId": "8ba1a119-7cc1-408b-ad53-dee9ac762b8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"A movie about dealing with the problems with growing up and being true to yourself, Blue Juice is mind candy for those who like surfing and Cornwall. Sean Pertwee is the real star of this film, while the more famous Catherine Zeta Jones plays his girlfriend and Ewan Mcgregor plays his drug addicted pal.<br /><br />For those who don't like surfing or Cornwall in the slightest, you'll find that it takes a long time before the movie even hints at being interesting. The beginning is slow and spends too much time on long shots of only slightly interesting landscapes. Plus too many main characters leads to most of them being one dimensional. The plot is an interesting idea but because of the shallow characters you have no idea why they act in the situations they're put in.<br /><br />Only Ewan, Sean and Catherine's characters make this a film worth being on videotape, which is why it was only released on videotape in the US after Ewan and Catherine reached mainstream fame.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.2 Displaying the shapes and dtypes of the first batch\n",
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNIbRDKrGDXI"
      },
      "outputs": [],
      "source": [
        "# Listing 11.3 Preprocessing our datasets with a TextVectorization layer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmZcy1GHrqA",
        "outputId": "fc05f0e0-3183-4074-881f-785d30fa8964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.4 Inspecting the output of our binary unigram dataset\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCLnGCoaH_iF"
      },
      "outputs": [],
      "source": [
        "# Listing 11.5 Our model-building utility\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIpNaV4QIPio",
        "outputId": "a1e68e6b-657d-4486-a100-b0ee55a62fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 24s 35ms/step - loss: 0.4081 - accuracy: 0.8274 - val_loss: 0.3012 - val_accuracy: 0.8770\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2739 - accuracy: 0.9000 - val_loss: 0.2977 - val_accuracy: 0.8846\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2436 - accuracy: 0.9166 - val_loss: 0.2981 - val_accuracy: 0.8866\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2358 - accuracy: 0.9238 - val_loss: 0.3119 - val_accuracy: 0.8890\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2207 - accuracy: 0.9277 - val_loss: 0.3290 - val_accuracy: 0.8892\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2136 - accuracy: 0.9311 - val_loss: 0.3358 - val_accuracy: 0.8880\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2140 - accuracy: 0.9343 - val_loss: 0.3491 - val_accuracy: 0.8872\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2195 - accuracy: 0.9352 - val_loss: 0.3519 - val_accuracy: 0.8850\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2095 - accuracy: 0.9358 - val_loss: 0.3590 - val_accuracy: 0.8848\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2109 - accuracy: 0.9373 - val_loss: 0.3665 - val_accuracy: 0.8832\n",
            "782/782 [==============================] - 15s 18ms/step - loss: 0.3013 - accuracy: 0.8840\n",
            "Test acc: 0.884\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.6 Training and testing the binary unigram model\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gvKWAJ3N-jjk",
        "SakN_k9mjI5a",
        "FkcB8Qi8lDjt",
        "HcPKmLzCoVLa",
        "EPc2huRLtgoF",
        "jb6PVnJWvoh8",
        "_ETwZpWU3Op4",
        "pdT4u9SiCcVk",
        "11uShIGFIplL"
      ],
      "name": "DeepLearning_book_11_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
