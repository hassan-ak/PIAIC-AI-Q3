{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_book_11_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 11 Deep learning for text"
      ],
      "metadata": {
        "id": "d1gK60vg-UUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and create datasets\n",
        "\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup\n",
        "\n",
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Listing 11.12 Preparing integer sequence datasets\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OADuNVPY3Nh",
        "outputId": "7eda900e-fb06-4482-c36d-5aebbe9fb2a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  31.5M      0  0:00:02  0:00:02 --:--:-- 31.5M\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.4 The Transformer architecture"
      ],
      "metadata": {
        "id": "xFJon4FPGazN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- “neural attention” could be used to build powerful sequence models that didn’t feature any recurrent layers or convolution layers"
      ],
      "metadata": {
        "id": "f2AsOzL6GdIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4.1 Understanding self-attention"
      ],
      "metadata": {
        "id": "BNJ9N01yGiYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- not all input information seen by a model is equally important to the task at hand, so models should “pay more attention” to some features and “pay less attention” to other features.\n",
        "- a smart embedding space would provide a different vector representation for a word depending on the other words surrounding it\n",
        "- The purpose of self-attention is to modulate the representation of a token by using the representations of related tokens in the sequence.\n",
        "- Steps\n",
        "  - Step 1 is to compute relevancy scores between the vector for “a word” and every other word in the sentence. These are our “attention scores.”\n",
        "  - Step 2 is to compute the sum of all word vectors in the sentence, weighted by our relevancy scores"
      ],
      "metadata": {
        "id": "7Z5GwkA4GmWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NumPy-like pseudocode\n",
        "\n",
        "  ```py\n",
        "  def self_attention(input_sequence):\n",
        "    output = np.zeros(shape=input_sequence.shape)\n",
        "    for i, pivot_vector in enumerate(input_sequence):\n",
        "      scores = np.zeros(shape=(len(input_sequence),))\n",
        "      for j, vector in enumerate(input_sequence):\n",
        "        scores[j] = np.dot(pivot_vector, vector.T)\n",
        "      scores /= np.sqrt(input_sequence.shape[1])\n",
        "      scores = softmax(scores)\n",
        "      new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
        "      for j, vector in enumerate(input_sequence):\n",
        "        new_pivot_representation += vector * scores[j]\n",
        "      output[i] = new_pivot_representation\n",
        "    return output\n",
        "  ```"
      ],
      "metadata": {
        "id": "yqUkau0qHOzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Keras built-in layer\n",
        "\n",
        "  ```py\n",
        "  num_heads = 4\n",
        "  embed_dim = 256\n",
        "  mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "  outputs = mha_layer(inputs, inputs, inputs)\n",
        "  ```"
      ],
      "metadata": {
        "id": "UFH1glVXI1pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GENERALIZED SELF-ATTENTION: THE QUERY-KEY-VALUE MODEL"
      ],
      "metadata": {
        "id": "QM0uYZj6JQk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer architecture was originally developed for machine translation, where you have to deal with two input sequences: the source sequence and the target sequence\n",
        "- A Transformer is a sequence-to-sequence model: it was designed to convert one sequence into another\n",
        "- self-attention mechanism\n",
        "  - “for each token in inputs (A), compute how much the token is related to every token in inputs (B), and use these scores to weight a sum of tokens from inputs (C).”\n",
        "  - “for each element in the query, compute how much the element is related to every key, and use these scores to weight a sum of values”\n",
        "- Transformer-style attention\n",
        "  - got a reference sequence that describes something you’re looking for: the query.\n",
        "  - got a body of knowledge that you’re trying to extract information from: the values. \n",
        "  - Each value is assigned a key that describes the value in a format that can be readily compared to a query. You simply match the query to the keys. Then you return a weighted sum of values."
      ],
      "metadata": {
        "id": "ELafpg7eMdAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4.2 Multi-head attention"
      ],
      "metadata": {
        "id": "w41oCVrFN4th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- “multi-head”\n",
        "  - the initial query, key, and value are sent through three independent sets of dense projections, resulting in three separate vectors. Each vector is processed via neural attention, and the different outputs are concatenated back together into a single output sequence. Each such subspace is called a “head.”\n",
        "- The presence of the learnable dense projections enables the layer to actually learn something, as opposed to being a purely stateless transformation that would require additional layers before or after it to be useful. In addition, having independent heads helps the layer learn different groups of features for each token, where features within one group are correlated with each other but are mostly independent from features in a different group."
      ],
      "metadata": {
        "id": "EFe7ImliQPgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4.3 The Transformer encoder"
      ],
      "metadata": {
        "id": "98SllFKbQsVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Factoring outputs into multiple independent spaces, adding residual connections, adding normalization layers—all of these are standard architecture patterns that one would be wise to leverage in any complex model. Together, these bells and whistles form the Transformer encoder"
      ],
      "metadata": {
        "id": "lr7WebDEQuPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer architecture\n",
        "  - a Transformer encoder that processes the source sequence\n",
        "  - a Transformer decoder that uses the source sequence to generate a translated version"
      ],
      "metadata": {
        "id": "wP4JGU5MSJTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.21 Transformer encoder implemented as a subclassed Layer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "         layers.Dense(embed_dim),])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "yHkXMWDeSWiP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Saving custom layers\n",
        "\n",
        "  ```py\n",
        "  config = layer.get_config()\n",
        "  new_layer = layer.__class__.from_config(config)\n",
        "  ```\n",
        "\n",
        "  ```py\n",
        "  layer = PositionalEmbedding(sequence_length, input_dim, output_dim)\n",
        "  config = layer.get_config()\n",
        "  new_layer = PositionalEmbedding.from_config(config)\n",
        "  ```\n",
        "\n",
        "- Loading a model\n",
        "\n",
        "  ```\n",
        "  model = keras.models.load_model(\n",
        "    filename, custom_objects={\"PositionalEmbedding\": PositionalEmbedding}\n",
        "  )\n",
        "  ```\n",
        "\n",
        "- LayerNormalization\n",
        "\n",
        "  ```py\n",
        "  def layer_normalization(batch_of_sequences):\n",
        "    mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    return (batch_of_sequences - mean) / variance\n",
        "  ```\n",
        "\n",
        "- BatchNormalization\n",
        "\n",
        "  ```py\n",
        "  def batch_normalization(batch_of_images):\n",
        "    mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "    variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "    return (batch_of_images - mean) / variance\n",
        "  ```"
      ],
      "metadata": {
        "id": "sJQszQ0PXQ0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# isting 11.22 Using the Transformer encoder for text classification\n",
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        " \n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZCXF8lUYxKQ",
        "outputId": "f5898c64-535e-4c66-d763-c935553c31c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, None, 256)        543776    \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,664,033\n",
            "Trainable params: 5,664,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.23 Training and evaluating the Transformer encoder based model\n",
        "callbacks = [\n",
        "  keras.callbacks.ModelCheckpoint(\n",
        "      \"transformer_encoder.keras\",\n",
        "      save_best_only=True)]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20,\n",
        " callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1fPUThJZrUv",
        "outputId": "27146242-e714-468c-a14f-699428703bcd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 49s 68ms/step - loss: 0.4944 - accuracy: 0.7688 - val_loss: 0.3110 - val_accuracy: 0.8698\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.3189 - accuracy: 0.8663 - val_loss: 0.3176 - val_accuracy: 0.8730\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.2450 - accuracy: 0.9030 - val_loss: 0.2813 - val_accuracy: 0.8934\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.1885 - accuracy: 0.9273 - val_loss: 0.4130 - val_accuracy: 0.8720\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.1550 - accuracy: 0.9402 - val_loss: 0.3497 - val_accuracy: 0.8836\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.1324 - accuracy: 0.9499 - val_loss: 0.4101 - val_accuracy: 0.8916\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.1142 - accuracy: 0.9580 - val_loss: 0.3874 - val_accuracy: 0.8836\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.1008 - accuracy: 0.9633 - val_loss: 0.5416 - val_accuracy: 0.8766\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.0872 - accuracy: 0.9693 - val_loss: 0.4541 - val_accuracy: 0.8748\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.0801 - accuracy: 0.9714 - val_loss: 0.5300 - val_accuracy: 0.8772\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0702 - accuracy: 0.9745 - val_loss: 0.5405 - val_accuracy: 0.8692\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.0663 - accuracy: 0.9777 - val_loss: 0.5688 - val_accuracy: 0.8694\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0539 - accuracy: 0.9813 - val_loss: 0.7240 - val_accuracy: 0.8700\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.0541 - accuracy: 0.9823 - val_loss: 0.5912 - val_accuracy: 0.8700\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.0477 - accuracy: 0.9834 - val_loss: 0.6835 - val_accuracy: 0.8684\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0422 - accuracy: 0.9858 - val_loss: 0.7734 - val_accuracy: 0.8668\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0392 - accuracy: 0.9868 - val_loss: 0.7265 - val_accuracy: 0.8694\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.0308 - accuracy: 0.9896 - val_loss: 0.8472 - val_accuracy: 0.8706\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0292 - accuracy: 0.9900 - val_loss: 0.9809 - val_accuracy: 0.8600\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0300 - accuracy: 0.9913 - val_loss: 0.9581 - val_accuracy: 0.8578\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc21b466950>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\n",
        "        \"TransformerEncoder\": TransformerEncoder})\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewFfkB3HaBko",
        "outputId": "1d7a7193-1b35-4fad-80a6-4e343811e8cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 19s 24ms/step - loss: 0.3151 - accuracy: 0.8759\n",
            "Test acc: 0.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### USING POSITIONAL ENCODING TO RE-INJECT ORDER INFORMATION"
      ],
      "metadata": {
        "id": "fzt245U2cPIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- give the model access to word-order information, we’re going to add the word’s position in the sentence to each word embedding. Our input word embeddings will have two components: the usual word vector, which represents the word independently of any specific context, and a position vector, which represents the position of the word in the current sentence"
      ],
      "metadata": {
        "id": "WnLqmPB7cRhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.24 Implementing positional embedding as a subclassed layer\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "    self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions \n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "O6uyNflLcdHt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PUTTING IT ALL TOGETHER: A TEXT-CLASSIFICATION TRANSFORMER"
      ],
      "metadata": {
        "id": "Nd9tqdHddW2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.25 Combining the Transformer encoder with positional embedding\n",
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        " \n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf1vVt4xdaHm",
        "outputId": "92bd7ddc-7b49-413c-c49c-b6928640e667"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posit  (None, None, 256)        5273600   \n",
            " ionalEmbedding)                                                 \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,817,633\n",
            "Trainable params: 5,817,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "  keras.callbacks.ModelCheckpoint(\n",
        "      \"full_transformer_encoder.keras\",\n",
        "      save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWJLtXZOd7Lq",
        "outputId": "1609e753-5af2-49d8-b7b2-847a4496eb8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 46s 70ms/step - loss: 0.4862 - accuracy: 0.7788 - val_loss: 0.2914 - val_accuracy: 0.8852\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.2361 - accuracy: 0.9079 - val_loss: 0.2602 - val_accuracy: 0.8962\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.1766 - accuracy: 0.9330 - val_loss: 0.2908 - val_accuracy: 0.8984\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.1464 - accuracy: 0.9466 - val_loss: 0.3861 - val_accuracy: 0.8764\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.1240 - accuracy: 0.9552 - val_loss: 0.3703 - val_accuracy: 0.8860\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.1117 - accuracy: 0.9617 - val_loss: 0.4857 - val_accuracy: 0.8782\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0962 - accuracy: 0.9682 - val_loss: 0.4185 - val_accuracy: 0.8910\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.0887 - accuracy: 0.9701 - val_loss: 0.4160 - val_accuracy: 0.8762\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 0.4678 - val_accuracy: 0.8886\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0698 - accuracy: 0.9761 - val_loss: 0.6233 - val_accuracy: 0.8816\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0618 - accuracy: 0.9795 - val_loss: 0.5830 - val_accuracy: 0.8784\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0574 - accuracy: 0.9797 - val_loss: 0.5078 - val_accuracy: 0.8874\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0528 - accuracy: 0.9826 - val_loss: 0.4957 - val_accuracy: 0.8836\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.0502 - accuracy: 0.9825 - val_loss: 0.6118 - val_accuracy: 0.8796\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0433 - accuracy: 0.9858 - val_loss: 0.6248 - val_accuracy: 0.8490\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0411 - accuracy: 0.9861 - val_loss: 0.6004 - val_accuracy: 0.8804\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0424 - accuracy: 0.9865 - val_loss: 0.6404 - val_accuracy: 0.8794\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0372 - accuracy: 0.9872 - val_loss: 0.7509 - val_accuracy: 0.8740\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0340 - accuracy: 0.9883 - val_loss: 0.8118 - val_accuracy: 0.8742\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0312 - accuracy: 0.9890 - val_loss: 0.6841 - val_accuracy: 0.8744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc19e1e0a50>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\n",
        "        \"TransformerEncoder\": TransformerEncoder,\n",
        "        \"PositionalEmbedding\": PositionalEmbedding}) \n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KouZJTd0eCyB",
        "outputId": "46c98ac9-0fd0-404c-ec1b-0bbd55f9aefc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 27s 34ms/step - loss: 0.2975 - accuracy: 0.8788\n",
            "Test acc: 0.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4.4 When to use sequence models over bag-of-words models"
      ],
      "metadata": {
        "id": "3VjLJrYscZ0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pay close attention to the ratio between the number of samples in your training data and the mean number of words per sample. \n",
        "- If that ratio is small—less than 1,500—then the bag-of-bigrams model will perform better\n",
        "- If that ratio is higher than 1,500, then you should go with a sequence model.\n",
        "- In other words, sequence models work best when lots of training data is available and when each sample is relatively short.\n"
      ],
      "metadata": {
        "id": "WeoXoKQZeZuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "SNRY9M4Of22Y"
      }
    }
  ]
}