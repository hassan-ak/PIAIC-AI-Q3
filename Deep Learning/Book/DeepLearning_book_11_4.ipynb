{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_book_11_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AWs6ikS-kLe1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 11 Deep learning for text"
      ],
      "metadata": {
        "id": "d1gK60vg-UUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.5 Beyond text classification: Sequence-to-sequence learning"
      ],
      "metadata": {
        "id": "xFJon4FPGazN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A sequence-to-sequence model takes a sequence as input and translates it into a different sequence\n",
        "- Machine translation\n",
        "  - Convert a paragraph in a source language to its equivalent in a target language.\n",
        "- Text summarization\n",
        "  - Convert a long document to a shorter version that retains the most important information.\n",
        "- Question answering\n",
        "  - Convert an input question into its answer.\n",
        "- Chatbots\n",
        "  - Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n",
        "- Text generation\n",
        "  - Convert a text prompt into a paragraph that completes the prompt."
      ],
      "metadata": {
        "id": "B63Mq1ZSr7c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- template behind sequence-to-sequence models\n",
        "  - During training\n",
        "    - An encoder model turns the source sequence into an intermediate representation.\n",
        "    - A decoder is trained to predict the next token i in the target sequence by looking at both previous to\u0002kens ( 0 to i - 1 ) and the encoded source sequence.\n",
        "  - During inference\n",
        "    - We obtain the encoded source sequence from the encoder.\n",
        "    - The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string \"[start]\" ), and uses them to predict the first real token in the sequence.\n",
        "    - The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string \"[end]\" )."
      ],
      "metadata": {
        "id": "9DgL1YrltZbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5.1 A machine translation example"
      ],
      "metadata": {
        "id": "Orv3beXOt8v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# English to spanish data base\n",
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzAiN27NuKAe",
        "outputId": "3b610b71-4a1c-4539-e711-5b18d0ef7ce7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-20 17:06:34--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.135.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.135.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-06-20 17:06:34 (136 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the file\n",
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = [] \n",
        "for line in lines:\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  spanish = \"[start] \" + spanish + \" [end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "o4h7SNaHuUOT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random data\n",
        "import random\n",
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDhAtgT6ujmn",
        "outputId": "56701d22-6b19-4d94-d465-8fc86cc5f109"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Tom is the one who broke the window yesterday.', '[start] Tom es el que rompió la ventana ayer. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and split\n",
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "CGXNNzgKuooL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.26 Vectorizing the English and Spanish text pairs\n",
        "import tensorflow as tf \n",
        "import string\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        " \n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(\n",
        "      lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "sD0L74s7vRpk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.27 Preparing datasets for the translation task\n",
        "batch_size = 64\n",
        " \n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return ({\n",
        "      \"english\": eng,\n",
        "      \"spanish\": spa[:, :-1],\n",
        "      }, \n",
        "      spa[:, 1:]) \n",
        "  \n",
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=1)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "aL2TI79djNaO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset outputs look like\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGR0Nm-Sj-Pd",
        "outputId": "df2b069e-0aba-417e-d518-5cba43d5eadf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5.2 Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "AWs6ikS-kLe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The simplest, naive way to use RNNs to turn a sequence into another sequence is to keep the output of the RNN at each time step\n",
        "  ```py\n",
        "  inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "  x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
        "  x = layers.LSTM(32, return_sequences=True)(x)\n",
        "  outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  ```"
      ],
      "metadata": {
        "id": "dxxmh61bmjeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- proper sequence-to-sequence setup\n",
        "  - first use an RNN (the encoder) to turn the entire source sequence into a single vector (or set of vectors)\n",
        "  - use this vector (or vectors) as the initial state of another RNN (the decoder), which would look at elements 0...N in the target sequence, and try to predict step N+1 in the target sequence."
      ],
      "metadata": {
        "id": "_M8MPLQ1ngpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.28 GRU-based encoder\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        " \n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "4qMMrLXfntRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.29 GRU-based decoder and the end-to-end model\n",
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step) "
      ],
      "metadata": {
        "id": "hXFbznkNoBfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cr4t1Mcpigh",
        "outputId": "57b7ef07-99a1-48fb-fa86-c64032540830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " english (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " spanish (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    3840000     ['english[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    3840000     ['spanish[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 1024)         7876608     ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " gru_1 (GRU)                    (None, None, 1024)   3938304     ['embedding_1[0][0]',            \n",
            "                                                                  'bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, None, 1024)   0           ['gru_1[0][0]']                  \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 15000)  15375000    ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 34,869,912\n",
            "Trainable params: 34,869,912\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.30 Training our recurrent sequence-to-sequence model\n",
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "seq2seq_rnn.fit(train_ds, epochs=5, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McxUL7wdoh8T",
        "outputId": "6afa25df-428f-4fb7-e9bd-2ad5390c61e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1302/1302 [==============================] - 134s 95ms/step - loss: 1.3238 - accuracy: 0.5257 - val_loss: 1.1519 - val_accuracy: 0.5683\n",
            "Epoch 2/5\n",
            "1302/1302 [==============================] - 120s 92ms/step - loss: 1.1777 - accuracy: 0.5757 - val_loss: 1.0752 - val_accuracy: 0.5984\n",
            "Epoch 3/5\n",
            "1302/1302 [==============================] - 119s 91ms/step - loss: 1.0858 - accuracy: 0.6077 - val_loss: 1.0350 - val_accuracy: 0.6179\n",
            "Epoch 4/5\n",
            "1302/1302 [==============================] - 119s 91ms/step - loss: 1.0377 - accuracy: 0.6323 - val_loss: 1.0208 - val_accuracy: 0.6293\n",
            "Epoch 5/5\n",
            "1302/1302 [==============================] - 119s 91ms/step - loss: 1.0074 - accuracy: 0.6506 - val_loss: 1.0171 - val_accuracy: 0.6355\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93887f3dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.31 Translating new sentences with our RNN encoder and decoder\n",
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\" \n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "    next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs] \n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHRBD_ztp8Hz",
        "outputId": "2286664c-0412-4fec-d90f-0a30779f0ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "I've corrected the mistake.\n",
            "[start] he [UNK] el error [end]\n",
            "-\n",
            "Something tells me Tom will be OK.\n",
            "[start] algo que me va a tom se está bien [end]\n",
            "-\n",
            "He saw the boy jump over the fence and run away.\n",
            "[start] Él vio a la [UNK] el [UNK] y se [UNK] a la [UNK] [end]\n",
            "-\n",
            "Why do the five yen coin and the fifty yen coin have holes in the center?\n",
            "[start] por qué los [UNK] la [UNK] y el [UNK] se [UNK] en el cinco de la [UNK] [end]\n",
            "-\n",
            "Tom visited Mary on October 20th.\n",
            "[start] tom le pasó a mary en el [UNK] de [UNK] de [UNK] [end]\n",
            "-\n",
            "We are teachers.\n",
            "[start] estamos [UNK] [end]\n",
            "-\n",
            "The explosion may have been caused by a gas leak.\n",
            "[start] la que se puede haber [UNK] un [UNK] de [UNK] [end]\n",
            "-\n",
            "I'm at the beach.\n",
            "[start] estoy en la playa [end]\n",
            "-\n",
            "Tom knows Mary won't tell John.\n",
            "[start] tom sabe que mary no se lo [UNK] a john [end]\n",
            "-\n",
            "Do you like to be alone?\n",
            "[start] te gusta estar solo [end]\n",
            "-\n",
            "Tell me what I should be watching for.\n",
            "[start] dime qué debería estar en punto [end]\n",
            "-\n",
            "What grade are you in?\n",
            "[start] qué estás haciendo [end]\n",
            "-\n",
            "Why do you want to work for our company?\n",
            "[start] por qué quieres hacer para nuestra casa [end]\n",
            "-\n",
            "He died last year.\n",
            "[start] murió el año pasado [end]\n",
            "-\n",
            "Would you stop babbling?\n",
            "[start] [UNK] de [UNK] [end]\n",
            "-\n",
            "He's a foreign exchange student.\n",
            "[start] Él es un estudiante de japonés [end]\n",
            "-\n",
            "The news took us by surprise.\n",
            "[start] las [UNK] nos [UNK] por él [end]\n",
            "-\n",
            "When I grow up, I want to be just like you.\n",
            "[start] cuando me quiero ser como tú [end]\n",
            "-\n",
            "There was no one in the room besides Tom and Mary.\n",
            "[start] no había una [UNK] en el cuarto de tom y mary [end]\n",
            "-\n",
            "I think you're the woman I've been waiting for all my life.\n",
            "[start] creo que eres la mujer que me [UNK] por todo el mundo [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- inference setup, while very simple, is rather inefficient, \n",
        "  - since we reprocess the entire source sentence and the entire generated target sentence every time we sample a new word. \n",
        "  - In a practical application, you’d factor the encoder and the decoder as two separate models, and your decoder would only run a single step at each token-sampling iteration, reusing its previous internal state"
      ],
      "metadata": {
        "id": "3oCCKm3prusN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- model could be improved\n",
        "  - use a deep stack of recurrent layers for both the encoder and the decoder\n",
        "  - use an LSTM instead of a GRU"
      ],
      "metadata": {
        "id": "Dot7hyUmr-hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- the RNN approach to sequence-to-sequence learning has a few fundamental limitations\n",
        "  - The source sequence representation has to be held entirely in the encoder state vector(s), which puts significant limitations on the size and complexity of the sentences you can translate. It’s a bit as if a human were translating a sentence entirely from memory, without looking twice at the source sentence while producing the translation.\n",
        "  - RNNs have trouble dealing with very long sequences, since they tend to progressively forget about the past—by the time you’ve reached the 100th token in either sequence, little information remains about the start of the sequence. That means RNN-based models can’t hold onto long-term context, which can be essential for translating long documents.\n"
      ],
      "metadata": {
        "id": "9BZakeo2sJme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5.3 Sequence-to-sequence learning with Transformer"
      ],
      "metadata": {
        "id": "rA3c6aBSslIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Neural attention enables Transformer models to successfully process sequences that are considerably longer and more complex than those RNNs can handle.\n",
        "- the Transformer encoder keeps the encoded representation in a sequence format: it’s a sequence of context-aware embedding vectors.\n",
        "- Transformer decoder reads tokens 0...N in the target sequence and tries to predict token N+1"
      ],
      "metadata": {
        "id": "rbvWLXjS09s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Listing 11.33 The TransformerDecoder\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Listing 11.34 TransformerDecoder method that generates a causal mask\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "# Listing 11.35 The forward pass of the TransformerDecoder\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ],
      "metadata": {
        "id": "Cr6TBEHP1n5y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mask the upper half of the pairwise attention matrix to prevent the model from paying any attention to information from the future"
      ],
      "metadata": {
        "id": "gTuzCTTm269G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.24 Implementing positional embedding as a subclassed layer\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "    self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions \n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "59siNcVq29uc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.21 Transformer encoder implemented as a subclassed Layer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "         layers.Dense(embed_dim),])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "ELVv9UdgDdYq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.36 End-to-end Transformer\n",
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "6hmqU74C39CF"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc99yAFXJVME",
        "outputId": "a5367095-b968-4100-d5e9-726af7bd8669"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " english (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " spanish (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding_14 (Posit  (None, None, 256)   3845120     ['english[0][0]']                \n",
            " ionalEmbedding)                                                                                  \n",
            "                                                                                                  \n",
            " positional_embedding_15 (Posit  (None, None, 256)   3845120     ['spanish[0][0]']                \n",
            " ionalEmbedding)                                                                                  \n",
            "                                                                                                  \n",
            " transformer_encoder_11 (Transf  (None, None, 256)   3155456     ['positional_embedding_14[0][0]']\n",
            " ormerEncoder)                                                                                    \n",
            "                                                                                                  \n",
            " transformer_decoder_11 (Transf  (None, None, 256)   5259520     ['positional_embedding_15[0][0]',\n",
            " ormerDecoder)                                                    'transformer_encoder_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, None, 256)    0           ['transformer_decoder_11[0][0]'] \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, None, 15000)  3855000     ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.37 Training the sequence-to-sequence Transformer\n",
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=5, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVxnQ9tbEJ67",
        "outputId": "4b919bf8-9896-4463-da9d-27feb2f1d345"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1302/1302 [==============================] - 100s 74ms/step - loss: 1.6442 - accuracy: 0.4270 - val_loss: 1.3207 - val_accuracy: 0.5090\n",
            "Epoch 2/5\n",
            "1302/1302 [==============================] - 95s 73ms/step - loss: 1.3459 - accuracy: 0.5304 - val_loss: 1.1728 - val_accuracy: 0.5652\n",
            "Epoch 3/5\n",
            "1302/1302 [==============================] - 95s 73ms/step - loss: 1.1901 - accuracy: 0.5806 - val_loss: 1.0808 - val_accuracy: 0.6023\n",
            "Epoch 4/5\n",
            "1302/1302 [==============================] - 95s 73ms/step - loss: 1.0987 - accuracy: 0.6140 - val_loss: 1.0402 - val_accuracy: 0.6241\n",
            "Epoch 5/5\n",
            "1302/1302 [==============================] - 95s 73ms/step - loss: 1.0488 - accuracy: 0.6368 - val_loss: 1.0163 - val_accuracy: 0.6342\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0b966de990>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 11.38 Translating new sentences with our Transformer model\n",
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhx-k1zQEaYr",
        "outputId": "e5c25646-8b5e-49ad-ca3a-3add3c354c91"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "The house has all the conveniences.\n",
            "[start] la casa está todo el [UNK] [end]\n",
            "-\n",
            "Tom has a pain in the shoulder.\n",
            "[start] tom tiene un dolor en el [UNK] [end]\n",
            "-\n",
            "She ignored him all day.\n",
            "[start] ella lo lo [UNK] todo el día [end]\n",
            "-\n",
            "I'm happy and satisfied.\n",
            "[start] soy feliz y feliz [end]\n",
            "-\n",
            "Speeding causes accidents.\n",
            "[start] [UNK] los niños [UNK] [end]\n",
            "-\n",
            "She advised him to lose weight.\n",
            "[start] le aconsejó que perder peso [end]\n",
            "-\n",
            "We're going out for lunch. Why don't you come along?\n",
            "[start] nos vamos a ver por qué no puedes ir bien [end]\n",
            "-\n",
            "You have to do it.\n",
            "[start] tienes que hacerlo lo lo hace [end]\n",
            "-\n",
            "Tom escaped from prison.\n",
            "[start] tom se puso de la universidad de lluvia [end]\n",
            "-\n",
            "The bathroom is dirty.\n",
            "[start] el baño está [UNK] [end]\n",
            "-\n",
            "You cannot buy happiness.\n",
            "[start] no puedes comprar la ropa [end]\n",
            "-\n",
            "Wear something warm. It's going to be cold this afternoon.\n",
            "[start] [UNK] algo que va a estar haciendo esta tarde [end]\n",
            "-\n",
            "The prize won't be given to her.\n",
            "[start] el reloj no se lo [UNK] para ella [end]\n",
            "-\n",
            "Not everybody is the same.\n",
            "[start] no todo el mundo es el mismo [end]\n",
            "-\n",
            "Let's just eat.\n",
            "[start] [UNK] comer [end]\n",
            "-\n",
            "America is made up of 50 states.\n",
            "[start] estados unidos está haciendo muchos estados unidos [end]\n",
            "-\n",
            "Don't make me hurt you.\n",
            "[start] no me lo [UNK] [end]\n",
            "-\n",
            "It's only a game.\n",
            "[start] es solo un juego [end]\n",
            "-\n",
            "They will agree on that.\n",
            "[start] ellos están de acuerdo en eso [end]\n",
            "-\n",
            "It is not easy to get rid of bad habits.\n",
            "[start] no es fácil trabajar de lo [UNK] [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "nvmuvnovKIoj"
      }
    }
  ]
}