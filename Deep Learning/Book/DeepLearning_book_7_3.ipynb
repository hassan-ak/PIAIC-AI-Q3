{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_book_7_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Working with Keras: A deep dive"
      ],
      "metadata": {
        "id": "OSuw8cy0ot7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Writing your own training and evaluation loops"
      ],
      "metadata": {
        "id": "I2aK6829bouV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- built-in fit() workflow is solely focused on supervised learning: \n",
        "  - a setup where there are known targets (also called labels or annotations) associated with your input data\n",
        "  - where you compute your loss as a function of these targets and the model’s predictions"
      ],
      "metadata": {
        "id": "vS6SOIkmw2tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- contents of a typical training loop\n",
        "  1. Run the forward pass (compute the model’s output) inside a gradient tape to obtain a loss value for the current batch of data.\n",
        "  2. Retrieve the gradients of the loss with regard to the model’s weights.\n",
        "  3. Update the model’s weights so as to lower the loss value on the current batch of data."
      ],
      "metadata": {
        "id": "iJUkyYrUxN0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.1 Training versus inference"
      ],
      "metadata": {
        "id": "24Lw4R-3xbee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- low-level training loop\n",
        "  - step 1 (the forward pass) \n",
        "    - was done via pre‐dictions = model(inputs, training=True)\n",
        "  - step 2 (retrieving the gradients computed by the gradient tape) \n",
        "    - was done via gradients = tape.gradient(loss, model.trainable_weights)"
      ],
      "metadata": {
        "id": "wlnTu28gxw3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trainable weights\n",
        "  - These are meant to be updated via backpropagation to minimize the loss of the model, such as the kernel and bias of a Dense layer.\n",
        "- Non-trainable weights\n",
        "  - These are meant to be updated during the forward pass by the layers that own them"
      ],
      "metadata": {
        "id": "LTt8g5vyycYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a supervised-learning training step\n",
        "'''\n",
        "def train_step(inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "    gradients = tape.gradients(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(model.trainable_weights, gradients))\n",
        "'''"
      ],
      "metadata": {
        "id": "k0X3_HO4yzou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.2 Low-level usage of metrics"
      ],
      "metadata": {
        "id": "wPyr5VXAzSmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simply call update_state(y_true,y_pred) for each batch of targets and predictions\n",
        "# use result() to query the current metric value\n",
        "\n",
        "from tensorflow import keras\n",
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result: {current_result:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdY405OMzdGG",
        "outputId": "8b60506d-5eba-4a1b-bb45-a90503585618"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# track the average of a scalar value\n",
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean() \n",
        "for value in values:\n",
        "  mean_tracker.update_state(value) \n",
        "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llP-h98rz303",
        "outputId": "c5b48eaf-8c81-45a4-fe1b-ce19f6ced921"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of values: 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.3 A complete training and evaluation loop"
      ],
      "metadata": {
        "id": "N4B189AA0HuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_mnist_model():\n",
        "  inputs = keras.Input(shape=(28 * 28,))\n",
        "  features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "  features = layers.Dropout(0.5)(features)\n",
        "  outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model\n",
        "\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]\n"
      ],
      "metadata": {
        "id": "0Nj0vUz70hJO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.4.3 A complete training and evaluation loop\n",
        "\n",
        "model = get_mnist_model()\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "loss_tracking_metric = keras.metrics.Mean()\n",
        "\n",
        "def train_step(inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    \n",
        "  logs = {}\n",
        "  \n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[metric.name] = metric.result()\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"loss\"] = loss_tracking_metric.result()\n",
        "  \n",
        "  return logs"
      ],
      "metadata": {
        "id": "ZKh4N9zA0S-n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 7.22 Writing a step-by-step training loop: resetting the metrics\n",
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "    loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "FzXHAfoA1qSv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 7.23 Writing a step-by-step training loop: the loop itself\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)\n",
        "  print(f\"Results at the end of epoch {epoch}\")\n",
        "  \n",
        "  for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fzREY8_12DY",
        "outputId": "92868db6-47cb-4b46-f693-70385f5cd784"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results at the end of epoch 0\n",
            "...sparse_categorical_accuracy: 0.9149\n",
            "...loss: 0.2891\n",
            "Results at the end of epoch 1\n",
            "...sparse_categorical_accuracy: 0.9535\n",
            "...loss: 0.1662\n",
            "Results at the end of epoch 2\n",
            "...sparse_categorical_accuracy: 0.9628\n",
            "...loss: 0.1406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- test_step() function is just a subset of the logic of train_step()\n",
        "  - It omits the code that deals with updating the weights of the model"
      ],
      "metadata": {
        "id": "FXYvUBh52nqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 7.24 Writing a step-by-step evaluation loop\n",
        "def test_step(inputs, targets):\n",
        "  predictions = model(inputs, training=False)\n",
        "  loss = loss_fn(targets, predictions)\n",
        "  \n",
        "  logs = {}\n",
        "  \n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[\"val_\" + metric.name] = metric.result()\n",
        "  \n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "  return logs\n",
        " \n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics() \n",
        "\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "  logs = test_step(inputs_batch, targets_batch) \n",
        "print(\"Evaluation results:\") \n",
        "\n",
        "for key, value in logs.items(): \n",
        "  print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfHOjZvC2vyN",
        "outputId": "42ed9c09-ba29-4d19-a120-ecb182e31f85"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9641\n",
            "...val_loss: 0.1396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.4 Make it fast with tf.function"
      ],
      "metadata": {
        "id": "ejt9rZsa3kBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It’s more performant to compile your TensorFlow code into a computation graph that can be globally optimized in a way that code interpreted line by line cannot"
      ],
      "metadata": {
        "id": "uESW-67-4Cpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 7.25 Adding a @tf.function decorator to our evaluation-step function\n",
        "@tf.function\n",
        "\n",
        "def test_step(inputs, targets):\n",
        "  predictions = model(inputs, training=False)\n",
        "  loss = loss_fn(targets, predictions)\n",
        "  \n",
        "  logs = {}\n",
        "  \n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[\"val_\" + metric.name] = metric.result()\n",
        "  \n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "  return logs\n",
        " \n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics() \n",
        "\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "  logs = test_step(inputs_batch, targets_batch) \n",
        "print(\"Evaluation results:\") \n",
        "\n",
        "for key, value in logs.items(): \n",
        "  print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTIG_rOo4Amb",
        "outputId": "e9ee545e-07bc-481b-8038-cc1ab5076921"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9641\n",
            "...val_loss: 0.1396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.5 Leveraging fit() with a custom training loop"
      ],
      "metadata": {
        "id": "ROKxKS0W4Tbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  provide a custom training step function and let the framework do the rest.\n",
        "- do this by overriding the train_step() method of the Model class. This is the function that is called by fit() for every batch of data."
      ],
      "metadata": {
        "id": "AZz03m_C4n1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Example\n",
        "  - We create a new class that subclasses keras.Model .\n",
        "  - We override the method train_step(self, data) . Its contents are nearly identical to what we used in the previous section. It returns a dictionary mapping metric names (including the loss) to their cur\u0002rent values.\n",
        "  - We implement a metrics property that tracks the model’s Metric instances. This enables the model to automatically call reset_state() on the model’s metrics at the start of each epoch and at the start of a call to evaluate() , so you don’t have to do it by hand.\n"
      ],
      "metadata": {
        "id": "VaG9UMG44v3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 7.26 Implementing a custom training step to use with fit()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "  def train_step(self, data):\n",
        "    inputs, targets = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = loss_fn(targets, predictions)\n",
        "    gradients = tape.gradient(loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "    loss_tracker.update_state(loss)\n",
        "    return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [loss_tracker] "
      ],
      "metadata": {
        "id": "rKGmD75-5EoV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        " \n",
        "model.compile(optimizer=keras.optimizers.RMSprop())\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw_dHFVa5-IA",
        "outputId": "1e42e600-36b0-4b15-e858-3c63ea6b5539"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.2989\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.1651\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1402\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f44ae12d3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After you’ve called com‐pile() , you get access to the following:\n",
        "  - self.compiled_loss\n",
        "    - The loss function you passed to compile()\n",
        "  - self.compiled_metrics\n",
        "    - A wrapper for the list of metrics you passed, which allows you to call\n",
        "      - self.compiled_metrics.update_state() to update all of your metrics at once.\n",
        "  - self.metrics\n",
        "    - The actual list of metrics you passed to compile()\n",
        "    - Note that it also includes a metric that tracks the loss\n",
        "    - similar to what we did manually with our loss_tracking_metric earlier."
      ],
      "metadata": {
        "id": "0cYnE2E-6RMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "class CustomModel(keras.Model):\n",
        "  def train_step(self, data):\n",
        "    inputs, targets = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.compiled_loss(targets, predictions)\n",
        "    gradients = tape.gradient(loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "    self.compiled_metrics.update_state(targets, predictions)\n",
        "    return {m.name: m.result() for m in self.metrics} \n",
        "\n",
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        " \n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW_oDMqi6QfL",
        "outputId": "9d1764f8-e8cd-498b-e527-f4d6ad4496e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.2967 - sparse_categorical_accuracy: 0.9125\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.1644 - sparse_categorical_accuracy: 0.9548\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.1402 - sparse_categorical_accuracy: 0.9631\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f44ada74d50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "KRBKCtbC7X5j"
      }
    }
  ]
}