{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning - Nasir Hussain - 2021/09/25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSuw8cy0ot7V"
      },
      "source": [
        "# 13 Best practices for the real world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HS0u3C3B5rp"
      },
      "source": [
        "## 7.0.3 Getting the most out of your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNYSSOYL30e9"
      },
      "source": [
        "### 7.0.3.1 Advanced architecture patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kk3dqw35-c"
      },
      "source": [
        "#### BATCH NORMALIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOJ78z113305"
      },
      "source": [
        "- Normalization\n",
        "  - make different samples seen by a machine-learning model more similar to each other\n",
        "\n",
        "    ```\n",
        "    normalized_data = (data - np.mean(data, axis=...)) / np.std(data, axis=...)\n",
        "    ```\n",
        "- Batch normalization\n",
        "  - a type of layer\n",
        "  - normalize data even as the mean and variance change over time during training\n",
        "  - works by internally maintaining an expo\u0002nential moving average of the batch-wise mean and variance of the data seen during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8P7Z4S-4aiI"
      },
      "source": [
        "#### DEPTHWISE SEPARABLE CONVOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcbXS4U-4l1F"
      },
      "source": [
        "- depthwise separable convolution layer\n",
        "  - make your model lighter (fewer trainable weight parameters)\n",
        "  - faster (fewer floating-point operations)\n",
        "  - perform a few percentage points better on its task\n",
        "  - performs a spatial convolution on each channel of its input, independently, before mixing output channels via a pointwise convolution\n",
        "    - separating the learning of spatial features and the learning of channel-wise features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA6E5GAW5gQ8"
      },
      "source": [
        "## 13.1 Getting the most out of your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW16MqJp6rKw"
      },
      "source": [
        "- hyperparameters\n",
        "  - How many layers should you stack? \n",
        "  - How many units or filters should go in each layer? \n",
        "  - Should you use relu as activation, or a different function? \n",
        "  - Should you use BatchNormalization after a given layer? \n",
        "  - How much dropout should you use?\n",
        "- process of optimizing hyperparameters\n",
        "  1. Choose a set of hyperparameters (automatically).\n",
        "  2. Build the corresponding model.\n",
        "  3. Fit it to your training data, and measure performance on the validation data.\n",
        "  4. Choose the next set of hyperparameters to try (automatically).\n",
        "  5. Repeat.\n",
        "  6. Eventually, measure performance on your test data.\n",
        "- algorithm that analyzes the relationship between validation performance and various hyperparameter values to choose the next set of hyperparameters to evaluate.\n",
        "  - Bayesian optimization, \n",
        "  - genetic algorithms, \n",
        "  - simple random search\n",
        "- Updating hyperparameters\n",
        "  - The hyperparameter space is typically made up of discrete decisions and thus isn’t continuous or differentiable. Hence, you typically can’t do gradient descent in hyperparameter space. Instead, you must rely on gradient-free optimization techniques, which naturally are far less efficient than gradient descent.\n",
        "  - Computing the feedback signal of this optimization process (does this set of hyperparameters lead to a high-performing model on this task?) can be extremely expensive: it requires creating and training a new model from scratch on your dataset.\n",
        "  - The feedback signal may be noisy: if a training run performs 0.2% better, is that because of a better model configuration, or because you got lucky with the initial weight values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwGJGc7N8i0R"
      },
      "source": [
        "#### USING KERASTUNER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk3DRz5b4Cbp",
        "outputId": "2877ce16-bab6-480e-c674-24e9cb4be8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 61 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 81 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133 kB 4.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# istallation\n",
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKahyUEt4Phk"
      },
      "source": [
        "- lets you replace hard-coded hyperparameter values with a range of possible choices (search space of the hyperparameter tuning process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4WnmlV3E4kFz"
      },
      "outputs": [],
      "source": [
        "# Listing 13.1 A KerasTuner model-building function\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        " \n",
        "def build_model(hp):\n",
        "  units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(units, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "  ])\n",
        "  optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"])\n",
        "  return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rbMo7Gr5DxI",
        "outputId": "22e1fc28-8dd5-4dc3-f6e5-6a633b5c3f0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "# more modular and configurable approach\n",
        "# Listing 13.2 A KerasTuner HyperModel\n",
        "\n",
        "import kerastuner as kt\n",
        " \n",
        "class SimpleMLP(kt.HyperModel):\n",
        "  def __init__(self, num_classes):\n",
        "    self.num_classes = num_classes\n",
        "  def build(self, hp):\n",
        "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "    model = keras.Sequential([\n",
        "      layers.Dense(units, activation=\"relu\"),\n",
        "    layers.Dense(self.num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "    return model\n",
        " \n",
        "hypermodel = SimpleMLP(num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUBscxCT51_c"
      },
      "source": [
        "- tuner\n",
        "  - Pick a set of hyperparameter values\n",
        "  - Call the model-building function with these values to create a model\n",
        "  - Train the model and record its metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ri2tNnI85_Tl"
      },
      "outputs": [],
      "source": [
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=100,\n",
        "    executions_per_trial=2,\n",
        "    directory=\"mnist_kt_test\",\n",
        "    overwrite=True, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGS_Tv3F6eCE",
        "outputId": "37a7efa5-3e1a-4df8-fb35-4de821477795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
            "optimizer (Choice)\n",
            "{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n"
          ]
        }
      ],
      "source": [
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T-yA-hC6il5"
      },
      "source": [
        "##### Objective maximization and minimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "klT_cDWt7AYW"
      },
      "outputs": [],
      "source": [
        "objective = kt.Objective(\n",
        "    name=\"val_accuracy\",\n",
        "    direction=\"max\")\n",
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective=objective,\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory=\"mnist_kt_test\",\n",
        "    overwrite=True, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca0SyxvV6-zt",
        "outputId": "ddea1ffa-1fb8-4ace-f891-16c0d2767eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 59s]\n",
            "val_accuracy: 0.9750000238418579\n",
            "\n",
            "Best val_accuracy So Far: 0.9750500023365021\n",
            "Total elapsed time: 00h 12m 15s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ],
      "source": [
        "# launch search\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "x_train_full = x_train[:]\n",
        "y_train_full = y_train[:]\n",
        "num_val_samples = 10000\n",
        "x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\n",
        "y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n",
        "callbacks = [\n",
        "  keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
        "]\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    batch_size=128, \n",
        "    epochs=100,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Iws8xqs_74hC"
      },
      "outputs": [],
      "source": [
        "# Listing 13.3 Querying the best hyperparameter configurations\n",
        "top_n = 4\n",
        "best_hps = tuner.get_best_hyperparameters(top_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c3GN1pg58PqS"
      },
      "outputs": [],
      "source": [
        "def get_best_epoch(hp):\n",
        "  model = build_model(hp)\n",
        "  callbacks=[\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", mode=\"min\", patience=10)\n",
        "  ]\n",
        "  history = model.fit(\n",
        "      x_train, y_train,\n",
        "      validation_data=(x_val, y_val),\n",
        "      epochs=15,\n",
        "      batch_size=128,\n",
        "      callbacks=callbacks)\n",
        "  val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "  best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
        "  print(f\"Best epoch: {best_epoch}\")\n",
        "  return best_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zvzIBnY48648",
        "outputId": "40864efb-2581-41db-e3eb-d62efdfa8406"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef get_best_trained_model(hp):\\n  best_epoch = get_best_epoch(hp)\\n  model.fit(\\n      x_train_full, y_train_full,\\n      batch_size=128, \\n      epochs=int(best_epoch * 1.2))\\n  return model\\n \\nbest_models = []\\n\\nfor hp in best_hps:\\n  model = get_best_trained_model(hp)\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train on the full dataset\n",
        "'''\n",
        "def get_best_trained_model(hp):\n",
        "  best_epoch = get_best_epoch(hp)\n",
        "  model.fit(\n",
        "      x_train_full, y_train_full,\n",
        "      batch_size=128, \n",
        "      epochs=int(best_epoch * 1.2))\n",
        "  return model\n",
        " \n",
        "best_models = []\n",
        "\n",
        "for hp in best_hps:\n",
        "  model = get_best_trained_model(hp)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoB5c0v39QIZ"
      },
      "source": [
        "#### THE ART OF CRAFTING THE RIGHT SEARCH SPACE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQevgYFi-Rat"
      },
      "source": [
        "#### THE FUTURE OF HYPERPARAMETER TUNING: AUTOMATED MACHINE LEARNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-sIdtcH-cgp"
      },
      "source": [
        "### 13.1.2 Model ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni7xFZ2VLpbK"
      },
      "source": [
        "- Ensembling consists of pooling together the predictions of a set of different models to produce better pre\u0002dictions.\n",
        "- Ensembling relies on the assumption that different well-performing models trained independently are likely to be good for different reasons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPFIbx1KCWDP"
      },
      "source": [
        "## 13.2 Scaling-up model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8UKTwTICrbT"
      },
      "source": [
        "### 13.2.1 Speeding up training on GPU with mixed precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWslTR84CXti"
      },
      "source": [
        "#### UNDERSTANDING FLOATING-POINT PRECISION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wE9ku-BDQoo"
      },
      "source": [
        "- Precision is to numbers what resolution is to images\n",
        "-  levels of precision\n",
        "  - Half precision, or float16 , where numbers are stored on 16 bits\n",
        "  - Single precision, or float32 , where numbers are stored on 32 bits\n",
        "  - Double precision, or float64 , where numbers are stored on 64 bits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybV0qHWoDaEH"
      },
      "source": [
        "##### A note on floating-point encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mteJmgXdgnSO"
      },
      "source": [
        "```\n",
        "{sign} * (2 ** ({exponent} - 127)) * 1.{mantissa}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cymn1uU0i7xB"
      },
      "source": [
        "- the resolution of floating-point numbers is in terms of the smallest distance between two arbitrary numbers that you’ll be able to safely process. \n",
        "  - In single precision, that’s around 1e-7.\n",
        "  - In double precision, that’s around 1e-16.\n",
        "  - in half precision, it’s only 1e-3.\n",
        "- The idea is to leverage 16- bit computations in places where precision isn’t an issue, and to work with 32-bit values in other places to maintain numerical stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aYu009sk41f"
      },
      "source": [
        "##### Beware of dtype defaults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSJSc86MlJaV",
        "outputId": "97e9aa45-f52e-454c-c4f7-9648dc6aef26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tf.float64"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np_array = np.zeros((2, 2))\n",
        "tf_tensor = tf.convert_to_tensor(np_array)\n",
        "tf_tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah2tBDIFi7k8",
        "outputId": "ff9a90e3-c3c2-4d8d-cf03-cd876282b7db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tf.float32"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np_array = np.zeros((2, 2))\n",
        "tf_tensor = tf.convert_to_tensor(np_array, dtype=\"float32\")\n",
        "tf_tensor.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BljhFGGlm5ly"
      },
      "source": [
        "#### MIXED-PRECISION TRAINING IN PRACTICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghP_YXoTm8I4"
      },
      "outputs": [],
      "source": [
        "# Turn on mixed precison\n",
        "from tensorflow import keras\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NFlgxU4nHL1"
      },
      "source": [
        "- Typically, most of the forward pass of the model will be done in float16 (with the exception of numerically unstable operations like softmax)\n",
        "- the weights of the model will be stored and updated in float32 ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2akPeJfnZZQ"
      },
      "source": [
        "### 13.2.2 Multi-GPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpB2Vnihnj-r"
      },
      "source": [
        "- two ways to distribute computation across multiple devices\n",
        "  - data parallelism\n",
        "    - a single model is replicated on multiple devices or multiple machines\n",
        "    - Each of the model replicas processes different batches of data, and then they merge their results\n",
        "  - model parallelism\n",
        "    - different parts of a single model run on different devices, processing a single batch of data together at the same time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1UBsGaoMvt"
      },
      "source": [
        "#### GETTING YOUR HANDS ON TWO OR MORE GPUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG4LBkS-oZPq"
      },
      "source": [
        "- Acquire 2–4 GPUs, mount them on a single machine (it will require a beefy power supply), and install CUDA drivers, cuDNN, etc. For most people, this isn’t the best option.\n",
        "- Rent a multi-GPU Virtual Machine (VM) on Google Cloud, Azure, or AWS. You’ll be able to use VM im\u0002ages with preinstalled drivers and software, and you’ll have very little setup overhead. This is likely the best option for anyone who isn’t training models 24/7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG2BOlnUpR38"
      },
      "source": [
        "#### SINGLE-HOST, MULTI-DEVICE SYNCHRONOUS TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B__QjHL0pY0q"
      },
      "outputs": [],
      "source": [
        "# machine with multiple GPUs\n",
        "'''\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(f\"Number of devices: {strategy.num_replicas_in_sync}\") \n",
        "with strategy.scope():\n",
        "  model = get_compiled_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      epochs=100,\n",
        "      validation_data=val_dataset,\n",
        "      callbacks=callbacks)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOniujHBqAUX"
      },
      "source": [
        "- MirroredStrategy\n",
        "  1. A batch of data (called global batch) is drawn from the dataset.\n",
        "  2. It gets split into four different sub-batches (called local batches). For instance, if the global batch has 512 samples, each of the four local batches will have 128 samples. Because you want local batches to be large enough to keep the GPU busy, the global batch size typically needs to be very large.\n",
        "  3. Each of the four replicas processes one local batch, independently, on its own device: they run a for\u0002ward pass, and then a backward pass. Each replica outputs a “weight delta” describing by how much to update each weight variable in the model, given the gradient of the previous weights with respect to the loss of the model on the local batch.\n",
        "  4. The weight deltas originating from local gradients are efficiently merged across the four replicas to obtain a global delta, which is applied to all replicas. Because this is done at the end of every step, the replicas always stay in sync: their weights are always equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOk7IV5T9bps"
      },
      "source": [
        "### 13.2.3 TPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WosdZSxf9f18"
      },
      "source": [
        "- application-specific integrated circuits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXXUCbdF9i1h"
      },
      "source": [
        "#### USING A TPU VIA GOOGLE COLAB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0mkMqh29yDm",
        "outputId": "26514ba0-435d-4680-a733-4ea4c7d4c3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.203.42:8470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.203.42:8470\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: grpc://10.71.203.42:8470\n"
          ]
        }
      ],
      "source": [
        "# connect notebook to tpu\n",
        "import tensorflow as tf\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "print(\"Device:\", tpu.master())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CwSfFJi-GFL",
        "outputId": "7a2ea625-e024-442b-e449-f9a8fd5323ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of replicas: 8\n"
          ]
        }
      ],
      "source": [
        "# Listing 13.4 Building a model in a TPUStrategy scope\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        " \n",
        "strategy = tf.distribute.TPUStrategy(tpu) \n",
        "print(f\"Number of replicas: {strategy.num_replicas_in_sync}\")\n",
        " \n",
        "def build_model(input_size):\n",
        "  inputs = keras.Input((input_size, input_size, 3))\n",
        "  x = keras.applications.resnet.preprocess_input(inputs)\n",
        "  x = keras.applications.resnet.ResNet50(weights=None, include_top=False, pooling=\"max\")(x)\n",
        "  outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(\n",
        "      optimizer=\"rmsprop\",\n",
        "      loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"])\n",
        "  return model\n",
        " \n",
        "with strategy.scope():\n",
        "  model = build_model(input_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAJ6zexB-ynT"
      },
      "source": [
        "- two options for data loading:\n",
        "  - Train from data that lives in the memory of the VM (not on disk). If your data is in a NumPy array, this is what you’re already doing. \n",
        "  - Store the data in a Google Cloud Storage (GCS) bucket, and create a dataset that reads the data directly from the bucket, without downloading locally. The TPU runtime can read data from GCS. This is your only option for datasets that are too large to live entirely in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yjV8odS_D9l",
        "outputId": "62ffc451-7f60-4ec3-e323-c1135d384879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 1.8675 - accuracy: 0.3872\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 1.7601 - accuracy: 0.4411\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 1.6064 - accuracy: 0.4732\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 1.6024 - accuracy: 0.4831\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 1.4300 - accuracy: 0.5401\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 1.3876 - accuracy: 0.5496\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 1.2698 - accuracy: 0.5959\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 1.3401 - accuracy: 0.5772\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 1.2667 - accuracy: 0.5860\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 1.1313 - accuracy: 0.6220\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e6febc550>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train from NumPy arrays in memory\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "model.fit(x_train, y_train, batch_size=1024,epochs=10) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_MwYIa2_N3x"
      },
      "source": [
        "##### Beware of I/O bottlenecks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-u8EX26_bYN"
      },
      "source": [
        "- If your dataset is small enough, you should keep it in the memory of the VM. You can do so by calling dataset.cache() on your dataset. That way, the data will only be read from GCS once.\n",
        "- If your dataset is too large to fit in memory, make sure to store it as TFRecord files—an efficient binary storage format that can be loaded very quickly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ8FDevm_kxO"
      },
      "source": [
        "#### LEVERAGING STEP FUSING TO IMPROVE TPU UTILIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tst621jLAYRD"
      },
      "source": [
        "- u need to train with very large batches to keep the TPU cores busy\n",
        "- working with enormous batches, you should make sure to increase your optimizer learning rate accordingly\n",
        "- step fusing\n",
        "  - keep reasonably sized batches while maintaining full TPU utilization\n",
        "  - run multiple steps of training during each TPU execution step\n",
        "  - do more work in between two round trips from the VM memory to the TPU"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepLearning_book_13_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
