{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deeplearning - Nasir Hussain - 2021/09/04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1gK60vg-UUI"
      },
      "source": [
        "# 11 Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7YCnenTCXrM"
      },
      "source": [
        "## 11.3 Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHpsLmQXfc7"
      },
      "source": [
        "### 11.3.3 Processing words as a sequence: The sequence model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY9fHMJdVSQi"
      },
      "source": [
        "- word order matters\n",
        "- sequence models are about\n",
        "  - exposed the model to raw word sequences and let it figure out such features on its own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huluDgjfTnXl"
      },
      "source": [
        "- To implement a sequence model\n",
        "  - start by representing your input samples as sequences of integer indices\n",
        "  - map each integer to a vector to obtain vector sequences\n",
        "  - feed these sequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RxbyVXqWTZl",
        "outputId": "f0882ca6-4f48-4262-c982-9717768b574b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  29.8M      0  0:00:02  0:00:02 --:--:-- 29.8M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3gD93t7WZyc",
        "outputId": "2aaad1af-4c50-4035-bfc4-970839fb5e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYa5aYuGWPgQ"
      },
      "source": [
        "#### A FIRST PRACTICAL EXAMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufcLTHOBWglh"
      },
      "outputs": [],
      "source": [
        "# Listing 11.12 Preparing integer sequence datasets\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUDPKnqSXswJ",
        "outputId": "e2bdce7c-6dd5-42a6-f64c-2f0fc6d02fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.13 A sequence model built on one-hot encoded vector sequences\n",
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GO3FUtDX3PO",
        "outputId": "111a8368-9820-4458-9593-73c44d4b87aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 174s 264ms/step - loss: 0.5358 - accuracy: 0.7491 - val_loss: 0.3799 - val_accuracy: 0.8398\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 173s 277ms/step - loss: 0.3599 - accuracy: 0.8715 - val_loss: 0.3221 - val_accuracy: 0.8802\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 175s 279ms/step - loss: 0.2877 - accuracy: 0.9003 - val_loss: 0.3176 - val_accuracy: 0.8874\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 175s 281ms/step - loss: 0.2366 - accuracy: 0.9185 - val_loss: 0.3166 - val_accuracy: 0.8880\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 176s 282ms/step - loss: 0.2043 - accuracy: 0.9312 - val_loss: 0.2984 - val_accuracy: 0.8940\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 176s 281ms/step - loss: 0.1937 - accuracy: 0.9369 - val_loss: 0.3170 - val_accuracy: 0.8816\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 176s 282ms/step - loss: 0.1615 - accuracy: 0.9477 - val_loss: 0.3262 - val_accuracy: 0.8792\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 177s 283ms/step - loss: 0.1455 - accuracy: 0.9538 - val_loss: 0.4457 - val_accuracy: 0.8754\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 177s 283ms/step - loss: 0.1314 - accuracy: 0.9577 - val_loss: 0.4449 - val_accuracy: 0.8712\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 176s 282ms/step - loss: 0.1125 - accuracy: 0.9656 - val_loss: 0.3501 - val_accuracy: 0.8902\n",
            "782/782 [==============================] - 103s 131ms/step - loss: 0.3410 - accuracy: 0.8767\n",
            "Test acc: 0.877\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.14 Training a first basic sequence model\n",
        "\n",
        "## Skip this part as it is too much time consuming\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ETrhiK2YELS"
      },
      "source": [
        "#### UNDERSTANDING WORD EMBEDDINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUbP8xqYMEw"
      },
      "source": [
        "- one-hot encoding\n",
        "  - different tokens you’re encoding are all independent from each other\n",
        "- Word embedding\n",
        "  - Word embeddings are vector representations of words that map human language into a structured geometric space.\n",
        "  - dense representations\n",
        "  - structured representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6WCETqFaspg"
      },
      "source": [
        "- two ways to obtain word embeddings\n",
        "  - Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
        "  - Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called pretrained word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-vNiQvnbJlB"
      },
      "source": [
        "#### LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfjB03KpbCqX",
        "outputId": "23bb8253-855f-4b95-bdfa-ab24edc1a843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  50.6M      0  0:00:01  0:00:01 --:--:-- 50.6M\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Donwload data and create datasets\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup\n",
        "\n",
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Listing 11.12 Preparing integer sequence datasets\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RF8cK3KXbb6C"
      },
      "outputs": [],
      "source": [
        "# Listing 11.15 Instantiating an Embedding layer\n",
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmEhmIA3dp95"
      },
      "source": [
        "- The Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hod3fzl-eOmn",
        "outputId": "7283961d-2547-4f09-a089-f97afdd0ce03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               73984     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BldryZrd6S9",
        "outputId": "4198d4e1-5a6d-4280-a849-4a8f40e463ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 39s 49ms/step - loss: 0.4799 - accuracy: 0.7849 - val_loss: 0.4546 - val_accuracy: 0.8374\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3172 - accuracy: 0.8851 - val_loss: 0.3305 - val_accuracy: 0.8750\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.2390 - accuracy: 0.9161 - val_loss: 0.3144 - val_accuracy: 0.8742\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.1953 - accuracy: 0.9304 - val_loss: 0.3381 - val_accuracy: 0.8824\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 31s 49ms/step - loss: 0.1660 - accuracy: 0.9433 - val_loss: 0.3895 - val_accuracy: 0.8802\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.1376 - accuracy: 0.9539 - val_loss: 0.3799 - val_accuracy: 0.8782\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.1171 - accuracy: 0.9625 - val_loss: 0.4059 - val_accuracy: 0.8816\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 31s 49ms/step - loss: 0.0962 - accuracy: 0.9691 - val_loss: 0.6081 - val_accuracy: 0.8342\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.0786 - accuracy: 0.9759 - val_loss: 0.4037 - val_accuracy: 0.8734\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.0659 - accuracy: 0.9801 - val_loss: 0.4719 - val_accuracy: 0.8776\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7a3201fb90>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Listing 11.16 Model that uses an Embedding layer trained from scratch\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKiEn4BEeWKs",
        "outputId": "c5dd7469-6cf8-42fb-9492-8fa532eac7b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 25s 31ms/step - loss: 0.3544 - accuracy: 0.8518\n",
            "Test acc: 0.852\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37JcDvKZeFR9"
      },
      "source": [
        "#### UNDERSTANDING PADDING AND MASKING "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpip-NRMjs6p",
        "outputId": "453924d5-c7b9-4c1b-b090-b531a7be371a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKibLtGderjU",
        "outputId": "72d5aebc-4879-4b52-937f-7adf08cba599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 44s 59ms/step - loss: 0.3959 - accuracy: 0.8171 - val_loss: 0.2757 - val_accuracy: 0.8814\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.2253 - accuracy: 0.9148 - val_loss: 0.2840 - val_accuracy: 0.8878\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.1599 - accuracy: 0.9417 - val_loss: 0.2956 - val_accuracy: 0.8910\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.1207 - accuracy: 0.9564 - val_loss: 0.3288 - val_accuracy: 0.8838\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0856 - accuracy: 0.9696 - val_loss: 0.4231 - val_accuracy: 0.8826\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.0647 - accuracy: 0.9775 - val_loss: 0.4813 - val_accuracy: 0.8666\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.0492 - accuracy: 0.9840 - val_loss: 0.4351 - val_accuracy: 0.8780\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.0329 - accuracy: 0.9885 - val_loss: 0.4505 - val_accuracy: 0.8636\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.0251 - accuracy: 0.9916 - val_loss: 0.5588 - val_accuracy: 0.8670\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.6357 - val_accuracy: 0.8708\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79af4fb310>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Listing 11.17 Using an Embedding layer with masking enabled\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwvtoSabjyum",
        "outputId": "8190ba89-15ce-49c7-c716-10a168c06230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 26s 30ms/step - loss: 0.2883 - accuracy: 0.8806\n",
            "Test acc: 0.881\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frZSd9Bed3xu"
      },
      "source": [
        "#### USING PRETRAINED WORD EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q61PJT13fP5S",
        "outputId": "69fd5efb-7acd-4860-cb71-64f746e8333e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-19 15:31:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-06-19 15:31:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-06-19 15:31:41--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.05MB/s    in 2m 40s  \n",
            "\n",
            "2022-06-19 15:34:21 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt2c-qSOfe1k",
        "outputId": "a851f0c1-5be5-42d3-e771-3405c64f449e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.18 Parsing the GloVe word-embeddings file\n",
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SpetegfVfksE"
      },
      "outputs": [],
      "source": [
        "# Listing 11.19 Preparing the GloVe word-embeddings matrix\n",
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PvX-GEYvfp30"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCviD7lVlNPI",
        "outputId": "909b4347-63c1-49f0-ba85-4fedb784004f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 64)               34048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOSw3eVxlSDb",
        "outputId": "d6aa10ec-a767-4a1b-efa4-66b1925ab8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 41s 54ms/step - loss: 0.5778 - accuracy: 0.6973 - val_loss: 0.4817 - val_accuracy: 0.7780\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4692 - accuracy: 0.7816 - val_loss: 0.4396 - val_accuracy: 0.7964\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.4181 - accuracy: 0.8112 - val_loss: 0.4464 - val_accuracy: 0.8008\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.3839 - accuracy: 0.8322 - val_loss: 0.3441 - val_accuracy: 0.8526\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.3566 - accuracy: 0.8476 - val_loss: 0.3389 - val_accuracy: 0.8536\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.3365 - accuracy: 0.8604 - val_loss: 0.3186 - val_accuracy: 0.8606\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3203 - accuracy: 0.8663 - val_loss: 0.3060 - val_accuracy: 0.8724\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.3026 - accuracy: 0.8753 - val_loss: 0.2926 - val_accuracy: 0.8750\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.2862 - accuracy: 0.8830 - val_loss: 0.2895 - val_accuracy: 0.8776\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 31s 49ms/step - loss: 0.2746 - accuracy: 0.8881 - val_loss: 0.2947 - val_accuracy: 0.8766\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79b0c72e10>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqLRVHPCftEL",
        "outputId": "0b792268-2880-45b6-fb9c-0a6bbe13e6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 30s 36ms/step - loss: 0.2968 - accuracy: 0.8725\n",
            "Test acc: 0.873\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepLearning_book_11_2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
