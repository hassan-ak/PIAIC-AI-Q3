{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deeplearning - Nasir Hussain - 2021/09/04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1gK60vg-UUI"
      },
      "source": [
        "# 11 Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvKWAJ3N-jjk"
      },
      "source": [
        "## 11.1 Natural language processing: The bird’s eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqGPvq94-YN7"
      },
      "source": [
        "-  NLP is about using machine learning and large datasets to give computers the ability not to understand language but to ingest a piece of language as input and return something useful, like predicting the following:\n",
        "  - text classification\n",
        "    - What’s the topic of this text?\n",
        "  - content filtering\n",
        "    - Does this text contain abuse?\n",
        "  - sentiment analysis\n",
        "    - Does this text sound positive or negative?\n",
        "  - language modeling\n",
        "    - What should be the next word in this incomplete sentence?\n",
        "  - translation\n",
        "    - How would you say this in German?\n",
        "  - summarization\n",
        "    - How would you summarize this article in one paragraph?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SakN_k9mjI5a"
      },
      "source": [
        "## 11.2 Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u63lwEnBkQJy"
      },
      "source": [
        "- Deep learning models only process numeric tensors\n",
        "- Vectorizing text is the process of transforming text into numeric tensors.\n",
        "  - First, you standardize the text to make it easier to process, such as by converting it to lowercase or removing punctuation.\n",
        "  - You split the text into units (called tokens), such as characters, words, or groups of words. This is called tokenization.\n",
        "  - indexing all tokens present in the data.\n",
        "  - You convert each such token into a numerical vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkcB8Qi8lDjt"
      },
      "source": [
        "### 11.2.1 Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWglYmpwlQhg"
      },
      "source": [
        "- Text standardization is a basic form of feature engineering that aims to erase encoding differences that you don’t want your model to have to deal with\n",
        "  - convert to lowercase and remove punctuation characters\n",
        "  - convert special characters to a standard form\n",
        "  - converting variations of a term into a single shared representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcPKmLzCoVLa"
      },
      "source": [
        "### 11.2.2 Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR8pmWtBq1qV"
      },
      "source": [
        "- break text into units to be vectorized (tokens)\n",
        "- Methods of tockenization\n",
        "  - Word-level tokenization\n",
        "    - Where tokens (units) are space-separated (or punctuation-separated) substrings.\n",
        "    - A variant of this is to further split words into subwords when applicable\n",
        "      - for instance, treating “staring” as “star+ing” or “called” as “call+ed.”\n",
        "  - N-gram tokenization\n",
        "    - Where tokens are groups of N consecutive words.\n",
        "    - a way to artificially inject a small amount of local word order information into the model \n",
        "      - For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n",
        "  - Character-level tokenization\n",
        "    - Where each character is its own token.\n",
        "      - used in specialized contexts, like text generation or speech recognition.\n",
        "- text-processing models\n",
        "  - sequence models\n",
        "    - those that care about word order\n",
        "    - use word-level tokenization\n",
        "  - bag-of-words models\n",
        "    - those that treat input words as a set, discarding their original order\n",
        "    - N-gram tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPc2huRLtgoF"
      },
      "source": [
        "#### Understanding N-grams and bag-of-words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NralbgOntyaZ"
      },
      "source": [
        "- N-grams are groups of N (or fewer) consecutive words that you can extract from a sentence\n",
        "\n",
        "  ```\n",
        "  the cat sat on the mat.\n",
        "\n",
        "  bag of 2-gram\n",
        "  {\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
        "  \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
        "  \n",
        "  bag of 3-gram\n",
        "  {\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\",\n",
        "  \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\",\n",
        "  \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
        "  ```\n",
        "- One-dimensional convnets, recurrent neural networks, and Transformers are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups, by looking at continuous word or character sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb6PVnJWvoh8"
      },
      "source": [
        "### 11.2.3 Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-Ie_6fzEx9"
      },
      "source": [
        "- encode each token into a numerical representation. \n",
        "  - do this in a stateless way\n",
        "    - hashing each token into a fixed binary vector\n",
        "  - build an index of all terms found in the training data\n",
        "    - assign a unique integer to each entry in the vocabulary.\n",
        "\n",
        "  ```python\n",
        "  #  restrict the vocabulary to only the top 20,000 or 30,000 most common words\n",
        "words\n",
        "  vocabulary = {} \n",
        "  for text in dataset:\n",
        "    text = standardize(text)\n",
        "    tokens = tokenize(text)\n",
        "    for token in tokens:\n",
        "      if token not in vocabulary:\n",
        "      vocabulary[token] = len(vocabulary)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwLqAlBzz-HH"
      },
      "source": [
        "- vector encoding\n",
        "\n",
        "  ```python\n",
        "  def one_hot_encode_token(token):\n",
        "    vector = np.zeros((len(vocabulary),))\n",
        "    token_index = vocabulary[token]\n",
        "    vector[token_index] = 1\n",
        "    return vector\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YkIgbVI1wHN"
      },
      "source": [
        "- while doing so always create a “out of vocabulary” index (abbreviated as OOV index)\n",
        "  - a catch-all for any token that wasn’t in the index.\n",
        "- When decoding a sequence of integers back into words, you’ll replace 1 with something like “[UNK]” \n",
        "- Special Token\n",
        "  - OOV token (index 1)\n",
        "  - mask token (index 0) for padding\n",
        "\n",
        "  ```\n",
        "  [5, 7, 124, 4,89] and [8, 34, 21]\n",
        "\n",
        "          ||\n",
        "          ||\n",
        "          \\/\n",
        "\n",
        "  [[5, 7, 124, 4, 89]\n",
        "  [8, 34, 21, 0, 0]] \n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ETwZpWU3Op4"
      },
      "source": [
        "### 11.2.4 Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea5pw-ma6X9K"
      },
      "source": [
        "- Python way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YS6WvZ93Wm1"
      },
      "outputs": [],
      "source": [
        "# python way to perform all above tasks\n",
        "\n",
        "import string\n",
        " \n",
        "class Vectorizer:\n",
        "  \n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        " \n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "\n",
        "dataset = [ \n",
        "  \"I write, erase, rewrite\",\n",
        "  \"Erase again, and then\",\n",
        "  \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RlKRGgWjviX",
        "outputId": "24d4f9bf-6498-470d-886b-82cee1648810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUDVAFfq58Gc",
        "outputId": "a7c6d9ed-c41b-451c-ffcf-10acf6765f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnU_ACYh6aP9"
      },
      "source": [
        "- tf way\n",
        "  - `TextVectorization` uses by default follwoing settings but can be altered\n",
        "  - convert to lowercase and remove punctua\u0002tion” for text standardization\n",
        "  - “split on whitespace” for tokenization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcAeJbma8S64"
      },
      "outputs": [],
      "source": [
        "# vectorization with tf with default seettings\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1q8B-EL7mWD"
      },
      "outputs": [],
      "source": [
        "# vectorization with tf with custom seettings\n",
        "import re \n",
        "import string \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        " \n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TdXB9ui9Iym"
      },
      "outputs": [],
      "source": [
        "# To index the vocabulary of a text corpus, just call the adapt() method of the layer \n",
        "  # with a Dataset object that yields strings\n",
        "  # just with a list of Python strings:\n",
        "dataset = [\n",
        "  \"I write, erase, rewrite\",\n",
        "  \"Erase again, and then\",\n",
        "  \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8tKU0jN9jkJ",
        "outputId": "b16b7351-4cf4-4fe8-85c6-c434236e7ea6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Listing 11.1 Displaying the vocabulary\n",
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQb5LWoR9pob",
        "outputId": "2153d3fa-d24d-4509-86bc-faa5979cc4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Sentence\n",
            "I write, rewrite, and still rewrite again\n",
            "Encoded Sentence\n",
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
            "Decoded Sentence\n",
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "# encode and then decode an example sentence\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(\"Actual Sentence\")\n",
        "print(test_sentence)\n",
        "print(\"Encoded Sentence\")\n",
        "print(encoded_sentence)\n",
        "print(\"Decoded Sentence\")\n",
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389LDDuD-VRR"
      },
      "source": [
        "#### Using the TextVectorization layer in a tf.data pipeline or as part of a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2i8AHZd-x3T"
      },
      "source": [
        "- two ways to use TextVectorization layer \n",
        "  - put it in the tf.data pipeline\n",
        "\n",
        "    ```python\n",
        "    int_sequence_dataset = string_dataset.map(\n",
        "      text_vectorization,\n",
        "      num_parallel_calls=4) \n",
        "    ```\n",
        "\n",
        "    -  happen synchronously with the rest of the model\n",
        "  - part of the model\n",
        "\n",
        "    ```python\n",
        "    text_input = keras.Input(shape=(), dtype=\"string\")\n",
        "    vectorized_text = text_vectorization(text_input)\n",
        "    embedded_input = keras.layers.Embedding(...)(vectorized_text)\n",
        "    output = ...\n",
        "    model = keras.Model(text_input, output) \n",
        "    ```\n",
        "    -  happen a-synchronously with the rest of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-OpsfjuA84P"
      },
      "source": [
        "- TextVectorization layer enables you to include text preprocessing right into your model, making it easier to deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7YCnenTCXrM"
      },
      "source": [
        "## 11.3 Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehjcOBVbB_Kp"
      },
      "source": [
        "- sequence models\n",
        "  - RNNs and Transformers\n",
        "  - those that care about word order\n",
        "  - use word-level tokenization\n",
        "- bag-of-words models\n",
        "  - those that treat input words as a set, discarding their original order\n",
        "  - N-gram tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdT4u9SiCcVk"
      },
      "source": [
        "### 11.3.1 Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqg39x0uCiO8",
        "outputId": "29ca1b70-6d49-4ad3-8730-91809c891c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9855k      0  0:00:08  0:00:08 --:--:-- 16.3M\n"
          ]
        }
      ],
      "source": [
        "# download an unzip data\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3noE22KgDN34"
      },
      "source": [
        "- 25,000 text files for training and another 25,000 for testing\n",
        "  - 12500 +ve\n",
        "  - 12500 -ve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6KqEermDSv-"
      },
      "outputs": [],
      "source": [
        "# remove un-necessary folder\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZphNFmp_DgxP",
        "outputId": "9cc6f760-50d3-4d2d-b50e-b31a4a340119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "# inspect data\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICOEtnsrEDfT"
      },
      "outputs": [],
      "source": [
        "# prepare a validation set \n",
        "  # by setting apart 20% of the training text files in a new directory, aclImdb/val\n",
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AH3xzSZENTN",
        "outputId": "544fb9d5-2819-4af2-d85c-96442038c133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# create text datasets\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvt0QwZaFJV_",
        "outputId": "8ba1a119-7cc1-408b-ad53-dee9ac762b8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"A movie about dealing with the problems with growing up and being true to yourself, Blue Juice is mind candy for those who like surfing and Cornwall. Sean Pertwee is the real star of this film, while the more famous Catherine Zeta Jones plays his girlfriend and Ewan Mcgregor plays his drug addicted pal.<br /><br />For those who don't like surfing or Cornwall in the slightest, you'll find that it takes a long time before the movie even hints at being interesting. The beginning is slow and spends too much time on long shots of only slightly interesting landscapes. Plus too many main characters leads to most of them being one dimensional. The plot is an interesting idea but because of the shallow characters you have no idea why they act in the situations they're put in.<br /><br />Only Ewan, Sean and Catherine's characters make this a film worth being on videotape, which is why it was only released on videotape in the US after Ewan and Catherine reached mainstream fame.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.2 Displaying the shapes and dtypes of the first batch\n",
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alaUoZ2mFhQV"
      },
      "source": [
        "### 11.3.2 Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGdnqugdFrO3"
      },
      "source": [
        "#### SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNIbRDKrGDXI"
      },
      "outputs": [],
      "source": [
        "# Listing 11.3 Preprocessing our datasets with a TextVectorization layer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmZcy1GHrqA",
        "outputId": "fc05f0e0-3183-4074-881f-785d30fa8964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.4 Inspecting the output of our binary unigram dataset\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCLnGCoaH_iF"
      },
      "outputs": [],
      "source": [
        "# Listing 11.5 Our model-building utility\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIpNaV4QIPio",
        "outputId": "a1e68e6b-657d-4486-a100-b0ee55a62fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 24s 35ms/step - loss: 0.4081 - accuracy: 0.8274 - val_loss: 0.3012 - val_accuracy: 0.8770\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2739 - accuracy: 0.9000 - val_loss: 0.2977 - val_accuracy: 0.8846\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2436 - accuracy: 0.9166 - val_loss: 0.2981 - val_accuracy: 0.8866\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2358 - accuracy: 0.9238 - val_loss: 0.3119 - val_accuracy: 0.8890\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2207 - accuracy: 0.9277 - val_loss: 0.3290 - val_accuracy: 0.8892\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2136 - accuracy: 0.9311 - val_loss: 0.3358 - val_accuracy: 0.8880\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2140 - accuracy: 0.9343 - val_loss: 0.3491 - val_accuracy: 0.8872\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2195 - accuracy: 0.9352 - val_loss: 0.3519 - val_accuracy: 0.8850\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2095 - accuracy: 0.9358 - val_loss: 0.3590 - val_accuracy: 0.8848\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2109 - accuracy: 0.9373 - val_loss: 0.3665 - val_accuracy: 0.8832\n",
            "782/782 [==============================] - 15s 18ms/step - loss: 0.3013 - accuracy: 0.8840\n",
            "Test acc: 0.884\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.6 Training and testing the binary unigram model\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11uShIGFIplL"
      },
      "source": [
        "#### BIGRAMS WITH BINARY ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz-SV4DsI84F"
      },
      "outputs": [],
      "source": [
        "# Listing 11.7 Configuring the TextVectorization layer to return bigrams\n",
        "text_vectorization = TextVectorization(\n",
        "  ngrams=2,\n",
        "  max_tokens=20000,\n",
        "  output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyNT0U0aJBNV",
        "outputId": "494b3830-b7b9-4add-d348-7997ffa7f74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 20ms/step - loss: 0.3714 - accuracy: 0.8461 - val_loss: 0.2775 - val_accuracy: 0.8944\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2321 - accuracy: 0.9195 - val_loss: 0.2803 - val_accuracy: 0.8978\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2022 - accuracy: 0.9333 - val_loss: 0.3154 - val_accuracy: 0.8956\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.1839 - accuracy: 0.9444 - val_loss: 0.3242 - val_accuracy: 0.8986\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.1815 - accuracy: 0.9460 - val_loss: 0.3338 - val_accuracy: 0.8978\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1772 - accuracy: 0.9499 - val_loss: 0.3503 - val_accuracy: 0.8968\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1805 - accuracy: 0.9484 - val_loss: 0.3591 - val_accuracy: 0.8964\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1681 - accuracy: 0.9535 - val_loss: 0.3686 - val_accuracy: 0.8952\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1702 - accuracy: 0.9524 - val_loss: 0.3795 - val_accuracy: 0.8958\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1673 - accuracy: 0.9546 - val_loss: 0.3830 - val_accuracy: 0.8952\n",
            "782/782 [==============================] - 11s 13ms/step - loss: 0.2712 - accuracy: 0.8987\n",
            "Test acc: 0.899\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.8 Training and testing the binary bigram model\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8EwSZbRKESM"
      },
      "source": [
        "#### BIGRAMS WITH TF-IDF ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zAmo0XaKEvJ"
      },
      "outputs": [],
      "source": [
        "# Listing 11.9 Configuring the TextVectorization layer to return token counts\n",
        "text_vectorization = TextVectorization(\n",
        " ngrams=2,\n",
        " max_tokens=20000,\n",
        " output_mode=\"count\"\n",
        ")\n",
        "\n",
        "# this might gives a problem to count some ir-relevent words such as the, a, is\n",
        "  # use normalization to mitigate this issue\n",
        "  # use only divide only normalization\n",
        "  # use TF-IDF stands for “term frequency, inverse document frequency.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_zZbEx1MFyA"
      },
      "source": [
        "##### Understanding TF-IDF normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9RaCRk5MTzP"
      },
      "source": [
        "- “term frequency,” how many times the term appears in the current document,\n",
        "- “document frequency,” which estimates how often the term comes up across the dataset.\n",
        "- tf-idf = tf/idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID8-l1TlMoi_"
      },
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "import math\n",
        "def tfidf(term, document, dataset):\n",
        "  term_freq = document.count(term)\n",
        "  doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
        "  return term_freq / doc_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKtN4kYbM2Bf"
      },
      "outputs": [],
      "source": [
        "# Listing 11.10 Configuring TextVectorization to return TF-IDF-weighted outputs\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hb79l8xM6pI",
        "outputId": "004d0eb9-731e-4a10-a772-1b8c128245a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 21ms/step - loss: 0.5291 - accuracy: 0.7556 - val_loss: 0.3190 - val_accuracy: 0.8712\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3474 - accuracy: 0.8515 - val_loss: 0.3245 - val_accuracy: 0.8648\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3176 - accuracy: 0.8602 - val_loss: 0.3276 - val_accuracy: 0.8720\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2989 - accuracy: 0.8679 - val_loss: 0.3453 - val_accuracy: 0.8534\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2886 - accuracy: 0.8837 - val_loss: 0.3529 - val_accuracy: 0.8768\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2680 - accuracy: 0.8910 - val_loss: 0.3454 - val_accuracy: 0.8626\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2498 - accuracy: 0.9008 - val_loss: 0.3355 - val_accuracy: 0.8730\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2389 - accuracy: 0.9056 - val_loss: 0.3581 - val_accuracy: 0.8466\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2317 - accuracy: 0.9044 - val_loss: 0.3568 - val_accuracy: 0.8602\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2291 - accuracy: 0.9060 - val_loss: 0.3535 - val_accuracy: 0.8782\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.3192 - accuracy: 0.8759\n",
            "Test acc: 0.876\n"
          ]
        }
      ],
      "source": [
        "# Listing 11.11 Training and testing the TF-IDF bigram model\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=1)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkl_Dvk7NRnS"
      },
      "source": [
        "##### Exporting a model that processes raw strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvYTLlRNNVdN"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wgYISKFNyhN",
        "outputId": "5de6dc80-0b2a-4f15-8170-1d494c9bb10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87.69 percent positive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        " [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data) \n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gvKWAJ3N-jjk",
        "SakN_k9mjI5a",
        "FkcB8Qi8lDjt",
        "HcPKmLzCoVLa",
        "EPc2huRLtgoF",
        "jb6PVnJWvoh8",
        "_ETwZpWU3Op4",
        "pdT4u9SiCcVk",
        "LGdnqugdFrO3",
        "11uShIGFIplL"
      ],
      "name": "DeepLearning_book_11_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
