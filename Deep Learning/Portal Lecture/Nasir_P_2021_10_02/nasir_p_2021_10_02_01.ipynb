{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning - Nasir Hussain - 2021/10/02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVIqKsm30-dr"
      },
      "source": [
        "# 12 Generative deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EikS00yB1DPR"
      },
      "source": [
        "- artificial intelligence to emulate\n",
        "  - passive tasks\n",
        "    - object recognition\n",
        "  - reactive tasks\n",
        "    - driving a car\n",
        "  - creative activities\n",
        "- more augmented intelligence than artificial intelligence\n",
        "- A large part of artistic creation consists of simple pattern recognition and technical skill\n",
        "- Our perceptual modalities, our language, and our artwork all have statistical structure. Learning this structure is what deep learning algorithms excel at.\n",
        "-  in the hands of a skilled artist, algorithmic generation can be steered to become meaningful and beautiful\n",
        "- Topics\n",
        "  - sequence data generation\n",
        "  - DeepDream\n",
        "  - image generation\n",
        "    - variational autoencoders\n",
        "    - generative adversarial networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNbNtlOn2L-n"
      },
      "source": [
        "## 12.1 Text generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo9VgNqB3W7v"
      },
      "source": [
        "- recurrent neural networks used to\n",
        "  - generate sequence data for\n",
        "    - text generation\n",
        "    - sequences of musical notes to generate new music\n",
        "    - brushstroke data generate paintings\n",
        "- Sequence data generation used for\n",
        "  - artistic content generation\n",
        "  - speech synthesis\n",
        "  - dialogue generation for chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXm2IP1c35iI"
      },
      "source": [
        "### 12.1.1 A brief history of generative deep learning for sequence generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alS-z4gf7ekb"
      },
      "source": [
        "- LSTM\n",
        "  - used early on to generate text character by character\n",
        "  - music generation\n",
        "- recurrent networks\n",
        "  - sequence data generation\n",
        "  - text and dialogue generation\n",
        "  - music generation\n",
        "  - speech synthesis.\n",
        "- recurrent mixture density networks \n",
        "  - to generate human-like handwriting \n",
        "- Transformer architecture\n",
        "  - supervised natural language processing\n",
        "  - generative sequence models\n",
        "  - language modeling (word-level text generation)\n",
        "    - GPT-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhTqCgjhHUhF"
      },
      "source": [
        "### 12.1.2 How do you generate sequence data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SUwBEQ6HWjj"
      },
      "source": [
        "- The universal way to generate sequence data in deep learning is to train a model to predict the next token or next few tokens in a sequence, using the previous tokens as input.\n",
        "- language model\n",
        "  - any network that can model the probability of the next token given the previous ones\n",
        "- trained language model (character-level neural language model)\n",
        "  - sample from it (generate new sequences)\n",
        "  - initial string of text (called conditioning data)\n",
        "  -  add the generated output back to the input data\n",
        "  - repeat the process many times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04JRtAc8ILRP"
      },
      "source": [
        "### 12.1.3 The importance of the sampling strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQXaopypO_T-"
      },
      "source": [
        "- way you choose the next token is crucially important\n",
        "  - greedy sampling\n",
        "    - consisting of always choosing the most likely next character.\n",
        "    - repetitive, predictable strings\n",
        "    -  sampling from a probability distribution\n",
        "      - one where a certain word has probability 1 and all others have probability 0.\n",
        "  - stochastic sampling\n",
        "    - it introduces randomness in the sampling process by sampling from the probability distribution for the next character \n",
        "- softmax output of the model is neat\n",
        "  - it allows even unlikely words to be sampled some of the time, generating more interesting-looking sentences and sometimes showing creativity by coming up with new, realistic-sounding sentences that didn’t occur in the training data\n",
        "  - it doesn’t offer a way to control the amount of randomness in the sampling process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Z3mwRVkG5G"
      },
      "source": [
        "- Why would you want more or less randomness?\n",
        "- pure random sampling\n",
        "  - maximum randomness\n",
        "  - higher entropy\n",
        "  - more surprising and creative sequences\n",
        "- greedy sampling\n",
        "  - No randomness\n",
        "  - lower entropy\n",
        "  - more predictable structure\n",
        "- softmax function\n",
        "  - constitutes an intermediate point between these two extremes\n",
        "- softmax temperature\n",
        "  - control the amount of stochasticity\n",
        "  - characterizes the entropy of the probability distribution used for sampling\n",
        "  - Higher temperatures result in sampling distributions of higher entropy\n",
        "  - lower temperature will result in less randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFRUdx3F05n_"
      },
      "outputs": [],
      "source": [
        "# Listing 12.1 Reweighting a probability distribution to a different temperature\n",
        "import numpy as np \n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "  distribution = np.log(original_distribution) / temperature\n",
        "  distribution = np.exp(distribution)\n",
        "  return distribution / np.sum(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4dBlPl-lgIt"
      },
      "source": [
        "### 12.1.4 Implementing text generation with Keras (IMDB movie review dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "129FuES9Dmve"
      },
      "source": [
        "- our language model will be a model of the style and topics of these movie reviews specifically, rather than a general model of the English language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNLSm64kDqq1"
      },
      "source": [
        "#### PREPARING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFw2ouFZDs11",
        "outputId": "d77ca862-b7fb-4531-e109-3fc81c1e0db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-03 20:18:14--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  33.5MB/s    in 2.4s    \n",
            "\n",
            "2022-06-03 20:18:17 (33.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Listing 12.2 Downloading and uncompressing the IMDB movie reviews dataset\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kViB2F_pECRu",
        "outputId": "5ca2b428-cb78-4038-e5a1-682e212ec154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# Listing 12.3 Creating a dataset from text files (one file = one sample)\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1_tAJxfEX9o"
      },
      "outputs": [],
      "source": [
        "# Listing 12.4 Preparing a TextVectorization layer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        " \n",
        "# only use first 100 words from the sequence\n",
        "sequence_length = 100\n",
        "# max words with higest occurance rest will be considered as \"UNK\"\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, \n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxFoYCXaFZ2v"
      },
      "outputs": [],
      "source": [
        "# Listing 12.5 Setting up a language modeling dataset\n",
        "# input samples are vectorized texts, \n",
        "# and corresponding targets are the same texts offset by one word.\n",
        "\n",
        "\n",
        "def prepare_lm_dataset(text_batch):\n",
        "  vectorized_sequences = text_vectorization(text_batch)\n",
        "  x = vectorized_sequences[:, :-1]\n",
        "  y = vectorized_sequences[:, 1:]\n",
        "  return x, y\n",
        " \n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySSUurlkGENy"
      },
      "source": [
        "#### A TRANSFORMER-BASED SEQUENCE-TO-SEQUENCE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eita_phCGQ9O"
      },
      "source": [
        "- We’ll train a model to predict a probability distribution over the next word in a sentence, given a number of initial words. \n",
        "- When the model is trained, we’ll feed it with a prompt, sample the next word, add that word back to the prompt, and repeat, until we’ve generated a short paragraph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utYvGxYqHV-u"
      },
      "source": [
        "- we could train a model that takes as input a sequence of N words and simply predicts word N+1\n",
        "  - N words are needed to be given as prompt but it is not usefull as we have 100 words in a sample\n",
        "  - Many of the training sequenece will be overlapping so lot of redundant work will be carried out by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MCa2E4pINx-"
      },
      "source": [
        "- To address these two issues we’ll use a sequence-to-sequence model\n",
        "  - we’ll feed sequences of N words into our model, and we’ll predict the sequence offset by one\n",
        "  - We’ll use causal masking to make sure that, for any i , the model will only be using words from 0 to i in order to predict the word i + 1. \n",
        "  - This means that we’re simultaneously training the model to solve N mostly overlapping but different problems: \n",
        "  - predicting the next words given a sequence of 1 <= i <= N prior words\n",
        "  - At generation time, even if you only prompt the model with a single word, it will be able to give you a probability distribution for the next possible words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqdFIOSlJuKk"
      },
      "source": [
        "- setup you can use for sequence-to-sequence learning in the general case: \n",
        "  - feed the source sequence into an encoder, and then feed both the encoded sequence and the target sequence into a decoder that tries to predict the same target sequence offset by one step. \n",
        "- When you’re doing text generation, there is no source sequence: \n",
        "  - you’re just trying to predict the next tokens in the target sequence given past tokens, which we can do using only the decoder. And thanks to causal padding, the decoder will only look at words 0...N to predict the word N+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yao_i57DGAep"
      },
      "outputs": [],
      "source": [
        "# PositionalEmbedding and TransformerDecoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKvqnnlRKr9_"
      },
      "outputs": [],
      "source": [
        "# Listing 12.6 A simple Transformer-based language model\n",
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        " \n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNvr8jNRK-UY"
      },
      "source": [
        "### 12.1.5 A text-generation callback with variable-temperature sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGaRHieMLXyu"
      },
      "outputs": [],
      "source": [
        "# Listing 12.7 The text-generation callback\n",
        "\n",
        "import numpy as np\n",
        " \n",
        "# Dict that maps word indices back to strings, to be used for text decoding\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        " \n",
        "#  Implements variable-temperature sampling from a probability distribution\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "  predictions = np.asarray(predictions).astype(\"float64\")\n",
        "  predictions = np.log(predictions) / temperature\n",
        "  exp_preds = np.exp(predictions)\n",
        "  predictions = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, predictions, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "  def __init__(\n",
        "      self,\n",
        "      prompt,\n",
        "      generate_length,\n",
        "      model_input_length,\n",
        "      temperatures=(1.,),\n",
        "      print_freq=1):\n",
        "    self.prompt = prompt\n",
        "    self.generate_length = generate_length\n",
        "    self.model_input_length = model_input_length\n",
        "    self.temperatures = temperatures\n",
        "    self.print_freq = print_freq\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if (epoch + 1) % self.print_freq != 0:\n",
        "      return\n",
        "    for temperature in self.temperatures:\n",
        "      print(\"== Generating with temperature\", temperature)\n",
        "      sentence = self.prompt\n",
        "      for i in range(self.generate_length):\n",
        "        tokenized_sentence = text_vectorization([sentence])\n",
        "        predictions = self.model(tokenized_sentence)\n",
        "        next_token = sample_next(predictions[0, i, :])\n",
        "        sampled_token = tokens_index[next_token]\n",
        "        sentence += \" \" + sampled_token\n",
        "      print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMupmYP8NRUg",
        "outputId": "e0cff5fa-99e6-4879-d7ea-1c29b888bf1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "  6/391 [..............................] - ETA: 2:35 - loss: 5.2924WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1891s vs `on_train_batch_end` time: 0.2117s). Check your callbacks.\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.0832== Generating with temperature 0.2\n",
            "This movie movie was does very have difficult heard to three be points [UNK] characters which and was its written still and a hilarious camera nancy work drew was barrymore worth was watching presented as what a may bad have until been the screen two the its films awesome few the parts\n",
            "== Generating with temperature 0.5\n",
            "This movie movie gives is me not stunned too me much just watching dolls it academy unlike award most was of i the was best at romantic the comedy severely at shields imdb a outside time of let the them person if could it not with be such scary an i offer\n",
            "== Generating with temperature 0.7\n",
            "This movie is was spoiler by all its kinds acting of a a holiday bullying and who the unknown overall people plot who plot went is wrong absolutely with location a and muddled terrible ripoff plot of holes tank that next deserves day to in the water animal battles pocket needed concentrate\n",
            "== Generating with temperature 1.0\n",
            "This movie was style not pretty as little much kids as it ridiculous is [UNK] captured fan the personally end than i the [UNK] cinema the do folks is instead that of it evil to teens be or unbearable independence to is enjoy that a we movie also those feel guys for\n",
            "== Generating with temperature 1.5\n",
            "This movie movie is has a been little made more by horror women feature noble of milestone her thereafter many filmed depth and going aid designed campaign to that barely they influenced had by whole el technique [UNK] the [UNK] results as of to similar let location us in international the political\n",
            "391/391 [==============================] - 169s 430ms/step - loss: 5.0832\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.7245== Generating with temperature 0.2\n",
            "This movie is is so a realistic waste film of it skateboarding the sets tube the to poverty an row [UNK] bad place story of with a a modern [UNK] [UNK] movie [UNK] of desert [UNK] you and should gives only an the animated appropriate state and of there [UNK] is [UNK]\n",
            "== Generating with temperature 0.5\n",
            "This movie movie was is a a very movie good and way the all acting time good faces the but world you pacing think of it people mostly who [UNK] should things have to taught happen this im story a basically simple brilliant story author and who his may defense not attorney\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is is [UNK] watchable highly and rated watching x of the [UNK] movie [UNK] this horse movie the it [UNK] is a about first as nightmare hell part the of twins a or conquered even a though very it early used career [UNK] to this work world together as\n",
            "== Generating with temperature 1.0\n",
            "This movie is is really above just average an beautiful end movie and of confused the and film uninteresting where [UNK] the actually film deserves obviously the to camera be full decades of now film id is like [UNK] a that surreal feels this like i low think the [UNK] [UNK] love\n",
            "== Generating with temperature 1.5\n",
            "This movie entertaining is the so entertaining good its about a a young living whos secret name who to [UNK] see the a [UNK] cop at named his streisand grave is the quite only similar a to deep a end terrifying of decently the wellknown film played noir in divided all pipe\n",
            "391/391 [==============================] - 169s 429ms/step - loss: 4.7245\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.5519== Generating with temperature 0.2\n",
            "This movie movie great has cinematography perhaps great tagline music this and movie it is was a all must for slap us in camera the work place like and george reaching [UNK] into last thinking but mixed for up an that hour this of movie my you time have we been supposed\n",
            "== Generating with temperature 0.5\n",
            "This movie is was far funny in as the mentioned love by type many actresses of like the angel 40s this or movie so or i even hope saved dedication this to movie laugh the but best addicted teacher to lasted someone 4 young years stars ago as the director double blake\n",
            "== Generating with temperature 0.7\n",
            "This movie is had about some expectations breasts and after then reading atlantic the ago short i it remember was seeing doing regular so films what right ever donna happened he at was her walking age in we every dont morning know we who loved knew him what and was then supposed\n",
            "== Generating with temperature 1.0\n",
            "This movie movie was is made a very movie low it budget isnt sets that in desperately making classify movies america i idiotic was b reading grade up z in grade the production dreadful made acting in illogical felix makeup and and action stunts romance nothing drama scares the no story gore\n",
            "== Generating with temperature 1.5\n",
            "This movie a is year absolutely [UNK] nothing smart [UNK] funny brewster for did a there real who hope was fck in it your but eyes what when happened your to dad be it a you marriage know trip youre and from joined cue what cards arts makes supports this the in\n",
            "391/391 [==============================] - 168s 429ms/step - loss: 4.5519\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.4414== Generating with temperature 0.2\n",
            "This movie is was the not only a movie little of [UNK] it [UNK] was you just almost seen kick every ass time the ive movie seen has worse this and movie ive is never a seen [UNK] such seen a four funny talk kids radio kids show with shouldnt a pick\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is is bad unbelievable it every is step that it way does when not the even characters call are me the they story dont is not one assembled unlikable in character it development is etc a look very at good each and scene every why time do the we\n",
            "== Generating with temperature 0.7\n",
            "This movie is screams one of of the the worst best [UNK] thing movies i i have have ever ever seen seen a my movie one about copy some i acting each this year movie after totally watching predictable it movie i wherein just the once best i thing saw i it\n",
            "== Generating with temperature 1.0\n",
            "This movie film had follows the a lives decent of novel israel [UNK] with [UNK] intelligence the agent actors who give played lights the up film they an just interesting not kelly to but blame for the this lead film years was after shot an so hour that into a this half\n",
            "== Generating with temperature 1.5\n",
            "This movie wasnt is great made too during in the any humor of in which particular at scene the but end all severely the [UNK] sort [UNK] of fire [UNK] the love sequel no and cinderella [UNK] does any not way really of [UNK] acting have by great his actors character a\n",
            "391/391 [==============================] - 169s 430ms/step - loss: 4.4414\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.3618== Generating with temperature 0.2\n",
            "This movie movie is is horrible so this i is am not working interested out in make it movies [UNK] luckily pulse i to see see list why site why run is believe run that mad is box there sheila are [UNK] four at women least and god are cat clearly series\n",
            "== Generating with temperature 0.5\n",
            "This movie is brings is one an of underrated the directors greatest poverty screen row time actor it robert comes arnold into still neurotic holds acting the than [UNK] the message [UNK] board sex provided chases a and [UNK] yuen metro [UNK] passage not once far there between is the not mother\n",
            "== Generating with temperature 0.7\n",
            "This movie movie tells one two of college the students two its famous about directors the whom creatures he behind help the peter killings allow and himself spiders this to is [UNK] a amateurish man homage after to yet beggars phantom belief [UNK] upon he seeing decides a to called take [UNK]\n",
            "== Generating with temperature 1.0\n",
            "This movie movie is falls clichéd bigger in the a bill white camp guy show getting and carey blah how you anybody get can to hang watch it a the remake french if film youre abbott looking [UNK] for will the be next laughing day laugh and track they the [UNK] pacing\n",
            "== Generating with temperature 1.5\n",
            "This movie is is so very stupid bad simply its the nothing end special result come cant on dream film from any its movie kind on of the slow ground trying breaking course up you but dont it ask reeks for its how title good cards movie and is i so actually\n",
            "391/391 [==============================] - 168s 429ms/step - loss: 4.3618\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.2998== Generating with temperature 0.2\n",
            "This movie movie made here me to think get the from movie the [UNK] 80s and to the the bad tagline live is action the and best the movie movie to of start the its movie worth sets seeing the something acting my and first the ever action [UNK] was in a\n",
            "== Generating with temperature 0.5\n",
            "This movie is was a bad great follow soundtrack up music to which our music jaw video [UNK] recording and it letters was to great dropping song in and the eddie end box of set the [UNK] stage boy design despite was the evident hype car and chase was note a to\n",
            "== Generating with temperature 0.7\n",
            "This movie is is so so so poorly good that this it movie will aint probably no the ever worst [UNK] movie this ever movie made is the one best of yet everything the about plot the or movie acting just get refers all to the character characters perfection are and horrific\n",
            "== Generating with temperature 1.0\n",
            "This movie was surprisingly actually good made in the hopes beginning of were the so same well about done five it times made in the 1990 [UNK] were were great still and given good the actors secret i story hated that the rise movie to and [UNK] dont all waste that your\n",
            "== Generating with temperature 1.5\n",
            "This movie movie makes was a so hilarious much yet its dont a see running a couple train of come pop young stars people and in walking this out movie they directed are a talking bit underground at cos different they stories seem just to not have only any been [UNK] better\n",
            "391/391 [==============================] - 168s 429ms/step - loss: 4.2998\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.2491== Generating with temperature 0.2\n",
            "This movie self is said amazing that the i creators believe were that hoping humanity to would get put this into movie the directing movie and the editing cgi in was this confusing movie stuff it that would took make to you more away may why not how do to this sit\n",
            "== Generating with temperature 0.5\n",
            "This movie is the glen chief ford of clint buffoon eastwood who henry has fonda tender played favorite by parts william otherwise h smarmy macy keep is in a the good rest script of weaving this his film elements to of his violence qualities one like might it criticize but it it\n",
            "== Generating with temperature 0.7\n",
            "This movie film has is no made stupid is guys geek just i brainless know with you [UNK] nothing below just aided happens and by against his his parents [UNK] after changing smoking your by head car [UNK] and through hes personal forced opinion into they chasing dubbed 5 guitar student is\n",
            "== Generating with temperature 1.0\n",
            "This movie was had completely an stuck abysmal with feel vengeance like i me [UNK] buddy [UNK] movies edward i norton knew and it cant was save i me wanted while to watching speak these on people disk screaming with good each dialogue others some will of give sorts it of a\n",
            "== Generating with temperature 1.5\n",
            "This movie one held of some the kind worst of of characteristics celluloid a posted [UNK] horrible ripped rating [UNK] from with me mr for [UNK] the degree killer im gone pretty mad much and and mad add max in the check last on 30 the shark shooting bait are 9 cut\n",
            "391/391 [==============================] - 170s 433ms/step - loss: 4.2491\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.2066== Generating with temperature 0.2\n",
            "This movie is is a what sweet it and i im think talking only about a is movie better cher than than your that average time starts you out cant with like the [UNK] old is lady just played a by part who so holds dont up like the because movie its\n",
            "== Generating with temperature 0.5\n",
            "This movie one has just ever moved been me so there boring is long so tour why de can palma anyone [UNK] not who at over all his there movies is miracles no and good will things that open can your make eyes your easy minds talent up and there waste may\n",
            "== Generating with temperature 0.7\n",
            "This movie film is was one very of [UNK] the films sex that scenes like are when no people one in kill the somebody city else club except is after about he a is sex married booth to at the the 3 airport attacks and him his depressed wife wife and also\n",
            "== Generating with temperature 1.0\n",
            "This movie excuse was me by jumping sat in well awe past that two ok people i in attended the in theatre mutiny who on must scene have the been pantheon made of to deeds be documentary sure nonsense to the go tragedy down of but the wherever bad directors is took\n",
            "== Generating with temperature 1.5\n",
            "This movie movie started on with and promising it ended because over we the expected web it show was into the a movie bmovie that it everybody takes has the [UNK] fist of to the present same a character 20 with and a far weak less plot samurai line given than the\n",
            "391/391 [==============================] - 170s 433ms/step - loss: 4.2066\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.1705== Generating with temperature 0.2\n",
            "This movie aka is depicts like everyday a life child of and [UNK] families the have character lived killings among this uncle excellent masters example stay [UNK] impressive and viewing both his of mother them a is monastery in he plan lives there with after each each other body throughout narration many\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is had true little guest time star with bob anyone hoskins whos himself buying allowed a me perfect to performances watch his on standup over paris his and head the usual best jokes part but comedy he stands just plenty as of is entertainment slowmotion and focuses the on\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is and bad put and 2007 a on director screen you no can equivalent call to at ask your the friends filmmakers or how persons your [UNK] husband this can has patton an we oddity believe continues that to this accept page that as it bitter went about into\n",
            "== Generating with temperature 1.0\n",
            "This movie is has a nearly real perfect reality title if i you say just slightly for better the but characters with are nothing forgettable more i than could secondly see this this movie movie 1 they wonder do which the added people cover that the will background be information sleeping during\n",
            "== Generating with temperature 1.5\n",
            "This movie is [UNK] absolutely digs nothing me at think all this of is these mostly things boring my but opinion it just is isnt absurd scary at and all thats it all it its tacky almost and a clowns shameless flick rip only off fails of at six the out fight\n",
            "391/391 [==============================] - 170s 434ms/step - loss: 4.1705\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.1391== Generating with temperature 0.2\n",
            "This movie movie is is better not and quite has actually a the abel whole [UNK] are so one over of the the top movie five art of directors the that film sits quality bonds film very but cute the is fight not scenes as more good emotion drama that at provides\n",
            "== Generating with temperature 0.5\n",
            "This movie movie is not pure a rubbish real film middle for stereotypes families in and [UNK] [UNK] from scenes action you [UNK] the van movie thrown was in terrible with the a typical bad hollywood performance hollywood by is the as bottom [UNK] mountain is poor [UNK] for only a one\n",
            "== Generating with temperature 0.7\n",
            "This movie was went going in to excellent the movie content the and acting good were script good was but very the slow dialogs scenes maybe were because believable me in what the they title made were this good movie should as be well the done lead crime character scene is [UNK]\n",
            "== Generating with temperature 1.0\n",
            "This movie movie is just quite brutal good it and is now somewhat probably similar the be [UNK] the and story kept of many ancient cult mythical movies rugged its starts a out spooky looking and like we a are rubber [UNK] monster bad and and there a is plain a to\n",
            "== Generating with temperature 1.5\n",
            "This movie was had a the gimmick bit of too a bad plot movie hackneyed it and was i pretty wrong sure the what typical i american give movie at a all 2 star out [UNK] of movies 10 in is what that you first did expect id anime seen to this\n",
            "391/391 [==============================] - 172s 438ms/step - loss: 4.1391\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabc9e57bd0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Listing 12.8 Fitting the language model\n",
        "\n",
        "# run on more epoch to get batter results say 200\n",
        "\n",
        "model.fit(lm_dataset, epochs=10, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV2zdtROS-Rz"
      },
      "source": [
        "- low temperature value results in very boring and repetitive text and can sometimes cause the generation process to get stuck in a loop.\n",
        "- higher temperatures, the generated text becomes more interesting, surprising, even creative.\n",
        "- very high temperature, the local structure starts to break down, and the output looks largely random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU8kmFc8TKeY"
      },
      "source": [
        "- training a bigger model, longer, on more data, you can achieve generated samples that look far more coherent and realistic than this one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkG5yQOsV-Jq"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepLearning_book_12_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
