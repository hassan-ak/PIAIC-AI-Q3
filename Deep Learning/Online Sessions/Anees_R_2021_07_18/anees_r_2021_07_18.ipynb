{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deeplearning - Anees Ahmad - 2021/07/18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1vv3o4Mwcq-"
      },
      "source": [
        "# 8 Introduction to deep learning for computer vision\n",
        "- convolutional neural networks\n",
        "  - convnets\n",
        "  - used universally in computer vision applications\n",
        "  - image-classification problems\n",
        "  - small training datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nqVyLGiQLMe"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ_KmAGl8SfU"
      },
      "source": [
        "## 8.1 Introduction to convnets\n",
        "- a basic convnet\n",
        "  - a stack of Conv2D and MaxPooling2D layers.\n",
        "  - convnet takes as input tensors of shape `(image_height, image_width,image_channels)`\n",
        "\n",
        "- build the model using the Functional API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PLLKCNi0-iSZ"
      },
      "outputs": [],
      "source": [
        "# Listing 8.1 Instantiating a small convnet\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "# define the input shape\n",
        "# as we are dealing with MNIST data we know it is a 28*28 pixels graysacale image\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "# a convent is stacks of Conv2D and MaxPooling2D layers\n",
        "# filter is actually the nodes/channels, karnel_size is the weight\n",
        "# same layer can be created using sequential class as follows\n",
        "  # model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# pool size defines the factor with which it scale down\n",
        "  # model.add(layers.MaxPooling2D((2, 2)))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlvyIHdHLiom"
      },
      "source": [
        "- Output of Conv2D and MaxPooling2D layer\n",
        "  - rank-3 tensor of shape `(height, width, channels)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "smGXrz65LcRU"
      },
      "outputs": [],
      "source": [
        "# next we have stacks of Dense layer, whihc actually takes 1D tensor as input\n",
        "# need to flattern the output of last Conv28 Layer\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIyur_dqCrg8",
        "outputId": "d88f9f0d-919e-40b7-d399-36a207c3e895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1152)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Listing 8.2 Displaying the modelâ€™s summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W3oBexaPFuo",
        "outputId": "7724c174-09a4-4418-aecd-2582c1420a77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 10s 9ms/step - loss: 0.1553 - accuracy: 0.9526\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0436 - accuracy: 0.9866\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0298 - accuracy: 0.9909\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0227 - accuracy: 0.9929\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0176 - accuracy: 0.9944\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f91d042f110>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Listing 8.3 Training the convnet on MNIST images\n",
        "from tensorflow.keras.datasets import mnist\n",
        " \n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQi_N352PTwC",
        "outputId": "336dddc8-4d98-4c35-e088-085887ba0120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0315 - accuracy: 0.9900\n",
            "Test accuracy: 0.990\n"
          ]
        }
      ],
      "source": [
        "# Listing 8.4 Evaluating the convnet\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW5B2HZCPnDX"
      },
      "source": [
        "- With out convents we have an accuracy of 97.8%\n",
        "- With convents we have accuracy of 99.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Sp2bPHQON-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMh-kNyr7mu-"
      },
      "source": [
        "## 8.1.1 The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tbLCeFM7ojj"
      },
      "source": [
        "- Dense vs Conolution Operation\n",
        "  - Dense layers learn global patterns in their input feature space\n",
        "  - convolution layers learn local patterns\n",
        "    - in the case of images, patterns found in small 2D windows of the inputs\n",
        "\n",
        "      ![](./snaps/8.1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlZEzEoEDIcH"
      },
      "source": [
        "- Properties of convents\n",
        "  - The patterns they learn are translation-invariant\n",
        "    - After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere. \n",
        "    - A densely connected model would have to learn the pattern anew if it appeared at a new location.\n",
        "    - This makes convnets data-efficient when processing images they need fewer training samples to learn representations that have generalization power.\n",
        "  - They can learn spatial hierarchies of patterns.\n",
        "    - A first convolution layer will learn small local patterns such as edges\n",
        "    - a second convolution layer will learn larger patterns made of the features of the first\n",
        "layers, and so on\n",
        "    - This allows convnets to efficiently learn increasingly complex and abstract visual concepts, because the visual world is fundamentally spatially hierarchical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa7ClYrVFah7"
      },
      "source": [
        "- Convolutions operate over rank-3 tensors called feature maps\n",
        "  - two spatial axes (height and width)\n",
        "  - a depth axis (also called the channels axis).\n",
        "- The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map.\n",
        "  - This output feature map is still a rank-3 tensor it has\n",
        "    - a width and a height. \n",
        "    - Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis stand for filters. \n",
        "    - Filters encode specific aspects of the input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRbOi7vOG8XD"
      },
      "source": [
        "- For istance take the following convet layer\n",
        "  - `x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(keras.Input(shape=(28, 28, 1)))`\n",
        "    - input feature map of size (28, 28, 1)\n",
        "    - output feature map of size (26, 26, 32)\n",
        "    - 32 filters over the input\n",
        "      - each channel conatins 26 Ã— 26 grid of values\n",
        "        - which is response map of the filter over the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeK5vEaoHEvj"
      },
      "source": [
        "- Convolutions are defined by two key parameters:\n",
        "  - Size of the patches extracted from the inputs\n",
        "  - Depth of the output feature map\n",
        "- convolution kernel\n",
        "- the output width and height may differ from the input width and height for two reasons:\n",
        "  - Border effects, which can be countered by padding the input feature map\n",
        "  - The use of strides, which Iâ€™ll define in a second\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPQtBpxgJLvd"
      },
      "source": [
        "#### UNDERSTANDING BORDER EFFECTS AND PADDING\n",
        "- A (5,5) input feature map to (3,3) output feature map\n",
        "\n",
        "    ![](./snaps/8.2.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZNldxRMKhqK"
      },
      "source": [
        "- If you want to get an output feature map with the same spatial dimensions as the input, you can use padding. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile.\n",
        "\n",
        "    ![](./snaps/8.3.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyuFYztzLCXJ"
      },
      "source": [
        "- padding argument\n",
        "  - \"valid\"\n",
        "    - no padding\n",
        "  - \"same\"\n",
        "    - pad in such a way as to have an output with the same width and height as the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb6qztIuLQKo"
      },
      "source": [
        "#### UNDERSTANDING CONVOLUTION STRIDES\n",
        "- Stride\n",
        "  -  distance between two successive windows is a parameter of the convolution\n",
        "\n",
        "  ![](./snaps/8.4.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6133Yj5MzAU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYtZAIQ5Mzml"
      },
      "source": [
        "### 8.1.2 The max-pooling operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOe_FP4aM2hl"
      },
      "source": [
        "- to aggressively downsample feature maps, much like strided convolutions\n",
        "- extract windows from the input feature maps and outputting the max value of each channel. \n",
        "- each channel is transformed via a hardcoded max tensor operation.\n",
        "- A big difference from convolution is that max pooling is usually done with 2 Ã— 2 windows and stride 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HiX9afARO_pV"
      },
      "outputs": [],
      "source": [
        "# Listing 8.5 An incorrectly structured convnet missing its max-pooling layers\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs9osYkAPI6_",
        "outputId": "6a8624b3-f196-4863-cc2f-2c4616c1e4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 22, 22, 128)       73856     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 61952)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                619530    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 712,202\n",
            "Trainable params: 712,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_no_max_pool.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmzAstQQQAW"
      },
      "source": [
        "- Problems with above model\n",
        "  - It isnâ€™t conducive to learning a spatial hierarchy of features. \n",
        "    - The 3 Ã— 3 windows in the third layer will only contain information coming from 7 Ã— 7 windows in the initial input. The high-level patterns learned by the convnet will still be very small with regard to the initial input, which may not be enough to learn to classify digits (try recognizing a digit by only looking at it through windows that are 7 Ã— 7 pixels!. We need the features from the last convolution layer to contain information about the totality of the input.\n",
        "  - The final feature map has 22 Ã— 22 Ã— 128 = 61,952 total coefficients per sample. This is huge. When you flatten it to stick a Dense layer of size 10 on top, that layer would have over half a million parameters. This is far too large for such a small model and would result in intense overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziOCAuRccqzx"
      },
      "source": [
        "- use of downsampling\n",
        "  - reduce the number of feature-map coefficients to process\n",
        "  - induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ven5yJIdc_gn"
      },
      "source": [
        "- Why Maxpoling works batter\n",
        "    - features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map (hence the term feature map), and itâ€™s more informative to look at the maximal presence of different features than at their average presence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHf7i-DWdIki"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepLearning-book-8.1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
