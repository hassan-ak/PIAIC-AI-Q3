{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deeplearning - Anees Ahmad - 2020/05/03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1gK60vg-UUI"
      },
      "source": [
        "# 11 Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvKWAJ3N-jjk"
      },
      "source": [
        "## 11.1 Natural language processing: The bird’s eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqGPvq94-YN7"
      },
      "source": [
        "-  NLP is about using machine learning and large datasets to give computers the ability not to understand language but to ingest a piece of language as input and return something useful, like predicting the following:\n",
        "  - text classification\n",
        "    - What’s the topic of this text?\n",
        "  - content filtering\n",
        "    - Does this text contain abuse?\n",
        "  - sentiment analysis\n",
        "    - Does this text sound positive or negative?\n",
        "  - language modeling\n",
        "    - What should be the next word in this incomplete sentence?\n",
        "  - translation\n",
        "    - How would you say this in German?\n",
        "  - summarization\n",
        "    - How would you summarize this article in one paragraph?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SakN_k9mjI5a"
      },
      "source": [
        "## 11.2 Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u63lwEnBkQJy"
      },
      "source": [
        "- Deep learning models only process numeric tensors\n",
        "- Vectorizing text is the process of transforming text into numeric tensors.\n",
        "  - First, you standardize the text to make it easier to process, such as by converting it to lowercase or removing punctuation.\n",
        "  - You split the text into units (called tokens), such as characters, words, or groups of words. This is called tokenization.\n",
        "  - indexing all tokens present in the data.\n",
        "  - You convert each such token into a numerical vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkcB8Qi8lDjt"
      },
      "source": [
        "### 11.2.1 Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWglYmpwlQhg"
      },
      "source": [
        "- Text standardization is a basic form of feature engineering that aims to erase encoding differences that you don’t want your model to have to deal with\n",
        "  - convert to lowercase and remove punctuation characters\n",
        "  - convert special characters to a standard form\n",
        "  - converting variations of a term into a single shared representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcPKmLzCoVLa"
      },
      "source": [
        "### 11.2.2 Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR8pmWtBq1qV"
      },
      "source": [
        "- break text into units to be vectorized (tokens)\n",
        "- Methods of tockenization\n",
        "  - Word-level tokenization\n",
        "    - Where tokens (units) are space-separated (or punctuation-separated) substrings.\n",
        "    - A variant of this is to further split words into subwords when applicable\n",
        "      - for instance, treating “staring” as “star+ing” or “called” as “call+ed.”\n",
        "  - N-gram tokenization\n",
        "    - Where tokens are groups of N consecutive words.\n",
        "    - a way to artificially inject a small amount of local word order information into the model \n",
        "      - For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n",
        "  - Character-level tokenization\n",
        "    - Where each character is its own token.\n",
        "      - used in specialized contexts, like text generation or speech recognition.\n",
        "- text-processing models\n",
        "  - sequence models\n",
        "    - those that care about word order\n",
        "    - use word-level tokenization\n",
        "  - bag-of-words models\n",
        "    - those that treat input words as a set, discarding their original order\n",
        "    - N-gram tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPc2huRLtgoF"
      },
      "source": [
        "#### Understanding N-grams and bag-of-words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NralbgOntyaZ"
      },
      "source": [
        "- N-grams are groups of N (or fewer) consecutive words that you can extract from a sentence\n",
        "\n",
        "  ```\n",
        "  the cat sat on the mat.\n",
        "\n",
        "  bag of 2-gram\n",
        "  {\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
        "  \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
        "  \n",
        "  bag of 3-gram\n",
        "  {\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\",\n",
        "  \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\",\n",
        "  \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
        "  ```\n",
        "- One-dimensional convnets, recurrent neural networks, and Transformers are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups, by looking at continuous word or character sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb6PVnJWvoh8"
      },
      "source": [
        "### 11.2.3 Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-Ie_6fzEx9"
      },
      "source": [
        "- encode each token into a numerical representation. \n",
        "  - do this in a stateless way\n",
        "    - hashing each token into a fixed binary vector\n",
        "  - build an index of all terms found in the training data\n",
        "    - assign a unique integer to each entry in the vocabulary.\n",
        "\n",
        "  ```python\n",
        "  #  restrict the vocabulary to only the top 20,000 or 30,000 most common words\n",
        "words\n",
        "  vocabulary = {} \n",
        "  for text in dataset:\n",
        "    text = standardize(text)\n",
        "    tokens = tokenize(text)\n",
        "    for token in tokens:\n",
        "      if token not in vocabulary:\n",
        "      vocabulary[token] = len(vocabulary)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwLqAlBzz-HH"
      },
      "source": [
        "- vector encoding\n",
        "\n",
        "  ```python\n",
        "  def one_hot_encode_token(token):\n",
        "    vector = np.zeros((len(vocabulary),))\n",
        "    token_index = vocabulary[token]\n",
        "    vector[token_index] = 1\n",
        "    return vector\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YkIgbVI1wHN"
      },
      "source": [
        "- while doing so always create a “out of vocabulary” index (abbreviated as OOV index)\n",
        "  - a catch-all for any token that wasn’t in the index.\n",
        "- When decoding a sequence of integers back into words, you’ll replace 1 with something like “[UNK]” \n",
        "- Special Token\n",
        "  - OOV token (index 1)\n",
        "  - mask token (index 0) for padding\n",
        "\n",
        "  ```\n",
        "  [5, 7, 124, 4,89] and [8, 34, 21]\n",
        "\n",
        "          ||\n",
        "          ||\n",
        "          \\/\n",
        "\n",
        "  [[5, 7, 124, 4, 89]\n",
        "  [8, 34, 21, 0, 0]] \n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ETwZpWU3Op4"
      },
      "source": [
        "### 11.2.4 Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea5pw-ma6X9K"
      },
      "source": [
        "- Python way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YS6WvZ93Wm1"
      },
      "outputs": [],
      "source": [
        "# python way to perform all above tasks\n",
        "\n",
        "import string\n",
        " \n",
        "class Vectorizer:\n",
        "  \n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        " \n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "\n",
        "dataset = [ \n",
        "  \"I write, erase, rewrite\",\n",
        "  \"Erase again, and then\",\n",
        "  \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RlKRGgWjviX",
        "outputId": "24d4f9bf-6498-470d-886b-82cee1648810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUDVAFfq58Gc",
        "outputId": "a7c6d9ed-c41b-451c-ffcf-10acf6765f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnU_ACYh6aP9"
      },
      "source": [
        "- tf way\n",
        "  - `TextVectorization` uses by default follwoing settings but can be altered\n",
        "  - convert to lowercase and remove punctua\u0002tion” for text standardization\n",
        "  - “split on whitespace” for tokenization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcAeJbma8S64"
      },
      "outputs": [],
      "source": [
        "# vectorization with tf with default seettings\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1q8B-EL7mWD"
      },
      "outputs": [],
      "source": [
        "# vectorization with tf with custom seettings\n",
        "import re \n",
        "import string \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        " \n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TdXB9ui9Iym"
      },
      "outputs": [],
      "source": [
        "# To index the vocabulary of a text corpus, just call the adapt() method of the layer \n",
        "  # with a Dataset object that yields strings\n",
        "  # just with a list of Python strings:\n",
        "dataset = [\n",
        "  \"I write, erase, rewrite\",\n",
        "  \"Erase again, and then\",\n",
        "  \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8tKU0jN9jkJ",
        "outputId": "b16b7351-4cf4-4fe8-85c6-c434236e7ea6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Listing 11.1 Displaying the vocabulary\n",
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQb5LWoR9pob",
        "outputId": "2153d3fa-d24d-4509-86bc-faa5979cc4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Sentence\n",
            "I write, rewrite, and still rewrite again\n",
            "Encoded Sentence\n",
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
            "Decoded Sentence\n",
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "# encode and then decode an example sentence\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(\"Actual Sentence\")\n",
        "print(test_sentence)\n",
        "print(\"Encoded Sentence\")\n",
        "print(encoded_sentence)\n",
        "print(\"Decoded Sentence\")\n",
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389LDDuD-VRR"
      },
      "source": [
        "#### Using the TextVectorization layer in a tf.data pipeline or as part of a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2i8AHZd-x3T"
      },
      "source": [
        "- two ways to use TextVectorization layer \n",
        "  - put it in the tf.data pipeline\n",
        "\n",
        "    ```python\n",
        "    int_sequence_dataset = string_dataset.map(\n",
        "      text_vectorization,\n",
        "      num_parallel_calls=4) \n",
        "    ```\n",
        "\n",
        "    -  happen synchronously with the rest of the model\n",
        "  - part of the model\n",
        "\n",
        "    ```python\n",
        "    text_input = keras.Input(shape=(), dtype=\"string\")\n",
        "    vectorized_text = text_vectorization(text_input)\n",
        "    embedded_input = keras.layers.Embedding(...)(vectorized_text)\n",
        "    output = ...\n",
        "    model = keras.Model(text_input, output) \n",
        "    ```\n",
        "    -  happen a-synchronously with the rest of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-OpsfjuA84P"
      },
      "source": [
        "- TextVectorization layer enables you to include text preprocessing right into your model, making it easier to deploy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gvKWAJ3N-jjk",
        "FkcB8Qi8lDjt",
        "HcPKmLzCoVLa",
        "EPc2huRLtgoF",
        "jb6PVnJWvoh8",
        "a7YCnenTCXrM",
        "pdT4u9SiCcVk",
        "LGdnqugdFrO3",
        "11uShIGFIplL"
      ],
      "name": "DeepLearning_book_11_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
